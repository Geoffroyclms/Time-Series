{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import quandl\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quandl.ApiConfig.api_key = \"ByAzesCp4TpPvjqYi4ay\" #Pour faire plus de 50 requetes par jours\n",
    "#start = datetime(2016,1,1)\n",
    "#end = datetime(2017,1,1)\n",
    "#s = \"CBS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logo_all=['AAPL','ABT','ACN','ATVI','ADBE','AMD','GOOGL','GOOG','AMZN','AAL','BLK','CBS','MSFT','FB']\n",
    "#logo=logo_all[1:]\n",
    "\n",
    "logo_all=pd.read_csv(r\"C:\\Users\\Geoffroy\\Desktop\\IT\\Projet Supélec\\Data_quandl\\logo_all.csv\")['Logo'].tolist()\n",
    "logo=logo_all.copy()\n",
    "logo.remove('AAPL')\n",
    "\n",
    "logo_LSTM=['AAPL_LSTM']\n",
    "for l in logo:\n",
    "    logo_LSTM.append(l+'_LSTM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_companies=pd.DataFrame()\n",
    "#df_companies['logo']=logo_all\n",
    "#df_companies['logo_LSTM']=logo_LSTM\n",
    "#df_companies.to_csv(r\"C:\\Users\\Geoffroy\\Desktop\\IT\\Projet Supélec\\Data_quandl\\Liste_entreprise.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MMM</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABMD</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ATVI</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AAP</th>\n",
       "      <th>...</th>\n",
       "      <th>AIV</th>\n",
       "      <th>AMAT</th>\n",
       "      <th>APTV</th>\n",
       "      <th>ADM</th>\n",
       "      <th>ARNC</th>\n",
       "      <th>AJG</th>\n",
       "      <th>AIZ</th>\n",
       "      <th>ATO</th>\n",
       "      <th>T</th>\n",
       "      <th>ADSK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-08</th>\n",
       "      <td>6.083995</td>\n",
       "      <td>4.910338</td>\n",
       "      <td>1.368321</td>\n",
       "      <td>3.321918</td>\n",
       "      <td>5.761588</td>\n",
       "      <td>2.513616</td>\n",
       "      <td>1.90827</td>\n",
       "      <td>2.444089</td>\n",
       "      <td>0.284957</td>\n",
       "      <td>19.665678</td>\n",
       "      <td>...</td>\n",
       "      <td>1.296928</td>\n",
       "      <td>1.109303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.40094</td>\n",
       "      <td>0.550842</td>\n",
       "      <td>1.507649</td>\n",
       "      <td>2.428563</td>\n",
       "      <td>1.198278</td>\n",
       "      <td>0.54329</td>\n",
       "      <td>4.112433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL       MMM       ABT      ABBV      ABMD       ACN  \\\n",
       "Date                                                                     \n",
       "2016-01-08  6.083995  4.910338  1.368321  3.321918  5.761588  2.513616   \n",
       "\n",
       "               ATVI      ADBE       AMD        AAP  ...       AIV      AMAT  \\\n",
       "Date                                                ...                       \n",
       "2016-01-08  1.90827  2.444089  0.284957  19.665678  ...  1.296928  1.109303   \n",
       "\n",
       "            APTV      ADM      ARNC       AJG       AIZ       ATO        T  \\\n",
       "Date                                                                         \n",
       "2016-01-08   NaN  4.40094  0.550842  1.507649  2.428563  1.198278  0.54329   \n",
       "\n",
       "                ADSK  \n",
       "Date                  \n",
       "2016-01-08  4.112433  \n",
       "\n",
       "[1 rows x 59 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################Price\n",
    "\n",
    "#df_all=pd.read_csv(r\"C:\\Users\\Geoffroy\\Desktop\\IT\\Projet Supélec\\Data_quandl\\Data_total_std.csv\",index_col='Date') #teste avec que une entreprise\n",
    "\n",
    "#Volatility sur 25 jours\n",
    "\n",
    "df_all=pd.read_csv(r\"C:\\Users\\Geoffroy\\Desktop\\IT\\Projet Supélec\\Data_quandl\\Data_std25_total.csv\",index_col='Date') #teste avec que une entreprise\n",
    "df_all=df_all[35:]\n",
    "#Log Return\n",
    "\n",
    "#df_all=pd.read_csv(r\"C:\\Users\\Geoffroy\\Desktop\\IT\\Projet Supélec\\Data_quandl\\Data_logreturn_total.csv\",index_col='Date') #teste avec que une entreprise\n",
    "\n",
    "\n",
    "df=df_all[:-3] #Les data - les 3 à prédire !\n",
    "date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()\n",
    "\n",
    "df_all.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tester, en supprimant les 35 premiers l'index n'est pas reset !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition des fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
    "\n",
    "        rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(\n",
    "            rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.hidden_layer = tf.placeholder(\n",
    "            tf.float32, (None, num_layers * 2 * size_layer)\n",
    "        )\n",
    "        self.outputs, self.last_state = tf.nn.dynamic_rnn(\n",
    "            drop, self.X, initial_state = self.hidden_layer, dtype = tf.float32\n",
    "        )\n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.cost = tf.reduce_mean(tf.square(self.Y - self.logits))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paramatre\n",
    "\n",
    "num_layers = 1\n",
    "size_layer = 128\n",
    "timestamp = 5\n",
    "epoch = 500\n",
    "dropout_rate = 0.7\n",
    "future_day = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_lstm(df,date_ori):\n",
    "    minmax = MinMaxScaler().fit(df.iloc[:, 1:].astype('float32'))\n",
    "    df_log = minmax.transform(df.iloc[:, 1:].astype('float32'))\n",
    "    df_log = pd.DataFrame(df_log)\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = Model(\n",
    "    0.01, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate)\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "        total_loss = 0\n",
    "        for k in range(0, df_log.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_log.shape[0] -1)\n",
    "            batch_x = np.expand_dims(\n",
    "                df_log.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_y = df_log.iloc[k + 1 : index + 1, :].values\n",
    "            last_state, _, loss = sess.run(\n",
    "                [modelnn.last_state, modelnn.optimizer, modelnn.cost],\n",
    "                feed_dict = {\n",
    "                    modelnn.X: batch_x,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.hidden_layer: init_value,\n",
    "                },\n",
    "            )\n",
    "            init_value = last_state\n",
    "            total_loss += loss\n",
    "        total_loss /= df_log.shape[0] // timestamp\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('epoch:', i + 1, 'avg loss:', total_loss)\n",
    "            \n",
    "    output_predict = np.zeros((df_log.shape[0] + future_day, df_log.shape[1]))\n",
    "    output_predict[0] = df_log.iloc[0]\n",
    "    upper_b = (df_log.shape[0] // timestamp) * timestamp\n",
    "    init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "    for k in range(0, (df_log.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    df_log.iloc[k : k + timestamp], axis = 0\n",
    "                ),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "    \n",
    "    out_logits, last_state = sess.run(\n",
    "        [modelnn.logits, modelnn.last_state],\n",
    "        feed_dict = {\n",
    "            modelnn.X: np.expand_dims(df_log.iloc[upper_b:], axis = 0),\n",
    "            modelnn.hidden_layer: init_value,\n",
    "        },\n",
    "    )\n",
    "    init_value = last_state\n",
    "    output_predict[upper_b + 1 : df_log.shape[0] + 1] = out_logits\n",
    "    df_log.loc[df_log.shape[0]] = out_logits[-1]\n",
    "    date_ori.append(date_ori[-1] + timedelta(days = 1))\n",
    "\n",
    "    for i in range(future_day - 1):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(df_log.iloc[-timestamp:], axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[df_log.shape[0]] = out_logits[-1]\n",
    "        df_log.loc[df_log.shape[0]] = out_logits[-1]\n",
    "        date_ori.append(date_ori[-1] + timedelta(days = 1))\n",
    "        \n",
    "    df_log = minmax.inverse_transform(output_predict)\n",
    "    date_ori = pd.Series(date_ori).dt.strftime(date_format = '%Y-%m-%d').tolist()\n",
    "    return df_log, date_ori\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'AAPL' in logo_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-2e6c344df6fd>:12: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D94CEF0DA0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:From <ipython-input-10-2e6c344df6fd>:16: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-10-2e6c344df6fd>:27: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-10-2e6c344df6fd>:29: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "epoch: 100 avg loss: 0.0015216469914058077\n",
      "epoch: 200 avg loss: 0.0023488109957390023\n",
      "epoch: 300 avg loss: 0.0018927190321846865\n",
      "epoch: 400 avg loss: 0.002285288642221866\n",
      "epoch: 500 avg loss: 0.0007197567741202779\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D94FAE9B70>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.002122810859107535\n",
      "epoch: 200 avg loss: 0.0010579335999194115\n",
      "epoch: 300 avg loss: 0.0024819687559532837\n",
      "epoch: 400 avg loss: 0.0011411622269262903\n",
      "epoch: 500 avg loss: 0.0021666839250093816\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D95177AD30>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0017867002854090013\n",
      "epoch: 200 avg loss: 0.0010294886562501472\n",
      "epoch: 300 avg loss: 0.0006885927224701797\n",
      "epoch: 400 avg loss: 0.0005183588699717228\n",
      "epoch: 500 avg loss: 0.0009451400385985739\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D951072A90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0014255298853018566\n",
      "epoch: 200 avg loss: 0.0011156622992865252\n",
      "epoch: 300 avg loss: 0.0011380821352328272\n",
      "epoch: 400 avg loss: 0.0011854159397204824\n",
      "epoch: 500 avg loss: 0.0012630391032584349\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D9546DD9E8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.003186927285768386\n",
      "epoch: 200 avg loss: 0.0028088977951005005\n",
      "epoch: 300 avg loss: 0.001464412521033266\n",
      "epoch: 400 avg loss: 0.0006375785313522149\n",
      "epoch: 500 avg loss: 0.0008452096364252563\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D95670F7F0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.004558188716672655\n",
      "epoch: 200 avg loss: 0.0018166485511564563\n",
      "epoch: 300 avg loss: 0.001512618347700673\n",
      "epoch: 400 avg loss: 0.0012071912772769697\n",
      "epoch: 500 avg loss: 0.0007289526941971609\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D957D0A7B8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.002308691698616665\n",
      "epoch: 200 avg loss: 0.0031810449440136403\n",
      "epoch: 300 avg loss: 0.002042329102167921\n",
      "epoch: 400 avg loss: 0.0019150935580623091\n",
      "epoch: 500 avg loss: 0.002199110774917675\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D95886DDD8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.001870443159484574\n",
      "epoch: 200 avg loss: 0.004015796562797684\n",
      "epoch: 300 avg loss: 0.0010959020265114592\n",
      "epoch: 400 avg loss: 0.0013804211134572\n",
      "epoch: 500 avg loss: 0.006507322820222103\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D95A8C2E48>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.005926410713934609\n",
      "epoch: 200 avg loss: 0.0029653125706030742\n",
      "epoch: 300 avg loss: 0.002525220342393272\n",
      "epoch: 400 avg loss: 0.0013820261866433277\n",
      "epoch: 500 avg loss: 0.0007671306757401908\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D95B0097B8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0008190147725001321\n",
      "epoch: 200 avg loss: 0.0010277291949711597\n",
      "epoch: 300 avg loss: 0.0009760753511823408\n",
      "epoch: 400 avg loss: 0.0018337159917313042\n",
      "epoch: 500 avg loss: 0.002698322387004737\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D95CD12CC0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0019342190207680687\n",
      "epoch: 200 avg loss: 0.0009757525729487258\n",
      "epoch: 300 avg loss: 0.0007354261651926208\n",
      "epoch: 400 avg loss: 0.0009279778239847234\n",
      "epoch: 500 avg loss: 0.0013262205475736042\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D953EF79E8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.009806470650226785\n",
      "epoch: 200 avg loss: 0.0022987782691703423\n",
      "epoch: 300 avg loss: 0.002775945852351326\n",
      "epoch: 400 avg loss: 0.0012837452951843214\n",
      "epoch: 500 avg loss: 0.0014160710894717148\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D95CE08B38>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.001338748197380674\n",
      "epoch: 200 avg loss: 0.0016416712912216194\n",
      "epoch: 300 avg loss: 0.001029247520196613\n",
      "epoch: 400 avg loss: 0.0008104994352546974\n",
      "epoch: 500 avg loss: 0.0011714835320737237\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D961130F28>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.002604506467928571\n",
      "epoch: 200 avg loss: 0.004292886305177633\n",
      "epoch: 300 avg loss: 0.0019396170312240045\n",
      "epoch: 400 avg loss: 0.0024727976185539248\n",
      "epoch: 500 avg loss: 0.0015679184584051224\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D957F75860>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.002401876488492187\n",
      "epoch: 200 avg loss: 0.0017276217360248619\n",
      "epoch: 300 avg loss: 0.0024476693560197753\n",
      "epoch: 400 avg loss: 0.0008508040648992296\n",
      "epoch: 500 avg loss: 0.0009096012811967643\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D963B00208>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0061677294763381055\n",
      "epoch: 200 avg loss: 0.001965719344489595\n",
      "epoch: 300 avg loss: 0.0038185211677428973\n",
      "epoch: 400 avg loss: 0.0036353344125688494\n",
      "epoch: 500 avg loss: 0.0021206043686680985\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D95A226F60>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0011551443312782794\n",
      "epoch: 200 avg loss: 0.00105797335460298\n",
      "epoch: 300 avg loss: 0.0006289381594079283\n",
      "epoch: 400 avg loss: 0.0007590548818705812\n",
      "epoch: 500 avg loss: 0.0005885001181875276\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D95E37D2E8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0017905995903818525\n",
      "epoch: 200 avg loss: 0.0017036998044779083\n",
      "epoch: 300 avg loss: 0.001179415308893352\n",
      "epoch: 400 avg loss: 0.0009659029195685626\n",
      "epoch: 500 avg loss: 0.0010788588299060632\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D968543EB8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0025034645289116796\n",
      "epoch: 200 avg loss: 0.0025651600888905754\n",
      "epoch: 300 avg loss: 0.000627081540809932\n",
      "epoch: 400 avg loss: 0.0011895074808547321\n",
      "epoch: 500 avg loss: 0.0011164813614461884\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D968C9FB38>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0026964286287119122\n",
      "epoch: 200 avg loss: 0.0008481288432927901\n",
      "epoch: 300 avg loss: 0.0016528242506051595\n",
      "epoch: 400 avg loss: 0.0009101727850257271\n",
      "epoch: 500 avg loss: 0.0007660424508733405\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D96A3719B0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0020049480724418046\n",
      "epoch: 200 avg loss: 0.0024831404842093193\n",
      "epoch: 300 avg loss: 0.0027462670313022834\n",
      "epoch: 400 avg loss: 0.003752349155547563\n",
      "epoch: 500 avg loss: 0.0029727379153547716\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D9636615C0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0028325051128999085\n",
      "epoch: 200 avg loss: 0.003704012204635594\n",
      "epoch: 300 avg loss: 0.003694775419717189\n",
      "epoch: 400 avg loss: 0.0014281834860173006\n",
      "epoch: 500 avg loss: 0.0013585243026395083\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D96D08D0B8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.005355010912018387\n",
      "epoch: 200 avg loss: 0.006094414014736877\n",
      "epoch: 300 avg loss: 0.001488335933237594\n",
      "epoch: 400 avg loss: 0.0010488097031109071\n",
      "epoch: 500 avg loss: 0.0011685727951867432\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D959F5BB38>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0036266354162714713\n",
      "epoch: 200 avg loss: 0.000869350413842384\n",
      "epoch: 300 avg loss: 0.0007093728297485015\n",
      "epoch: 400 avg loss: 0.0006561838766317937\n",
      "epoch: 500 avg loss: 0.0020734925004178783\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D964CB13C8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0011615162716464653\n",
      "epoch: 200 avg loss: 0.000995127638337803\n",
      "epoch: 300 avg loss: 0.0008544149260208206\n",
      "epoch: 400 avg loss: 0.0009009331134668747\n",
      "epoch: 500 avg loss: 0.0007370052546895366\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D966DBF390>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0014215373264864627\n",
      "epoch: 200 avg loss: 0.0010240097081613815\n",
      "epoch: 300 avg loss: 0.009722203049597354\n",
      "epoch: 400 avg loss: 0.001975100702302849\n",
      "epoch: 500 avg loss: 0.0013214757721135882\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D974D2EB38>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0017999242757612496\n",
      "epoch: 200 avg loss: 0.0011714383795869683\n",
      "epoch: 300 avg loss: 0.0018864612829929683\n",
      "epoch: 400 avg loss: 0.0014782122468061157\n",
      "epoch: 500 avg loss: 0.0013065059691526632\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D9763FB6D8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0010471646246993547\n",
      "epoch: 200 avg loss: 0.0016074697952334599\n",
      "epoch: 300 avg loss: 0.0007020972820770878\n",
      "epoch: 400 avg loss: 0.0015071159999168135\n",
      "epoch: 500 avg loss: 0.0005676193032310326\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D976B1E9B0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.00186153242286405\n",
      "epoch: 200 avg loss: 0.001497453689808026\n",
      "epoch: 300 avg loss: 0.0005674121868268182\n",
      "epoch: 400 avg loss: 0.000636717241880098\n",
      "epoch: 500 avg loss: 0.0009010057753920065\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D9782B00F0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0014376985759315033\n",
      "epoch: 200 avg loss: 0.0010018131703729619\n",
      "epoch: 300 avg loss: 0.0008824365855903842\n",
      "epoch: 400 avg loss: 0.001149139053316168\n",
      "epoch: 500 avg loss: 0.0008634722942492532\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D96B2DBB70>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0013584699845523573\n",
      "epoch: 200 avg loss: 0.0010563684780591804\n",
      "epoch: 300 avg loss: 0.0008088515502382315\n",
      "epoch: 400 avg loss: 0.0014478423666476278\n",
      "epoch: 500 avg loss: 0.0016838078282537601\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D962AB7E10>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0025585868243069854\n",
      "epoch: 200 avg loss: 0.001470529258719915\n",
      "epoch: 300 avg loss: 0.0033524207866369245\n",
      "epoch: 400 avg loss: 0.0006542482762214556\n",
      "epoch: 500 avg loss: 0.00048147285576162274\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D9631A3630>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0010003158052935951\n",
      "epoch: 200 avg loss: 0.0006754868182293936\n",
      "epoch: 300 avg loss: 0.0008643391156510916\n",
      "epoch: 400 avg loss: 0.0008264378773578516\n",
      "epoch: 500 avg loss: 0.0007832324947804935\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D97D44E0B8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0014470694316658314\n",
      "epoch: 200 avg loss: 0.0011818697494163644\n",
      "epoch: 300 avg loss: 0.000737890273261368\n",
      "epoch: 400 avg loss: 0.0013577912767160389\n",
      "epoch: 500 avg loss: 0.0014804615233228297\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D95D42DA20>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.002848115557418695\n",
      "epoch: 200 avg loss: 0.0024912589910136853\n",
      "epoch: 300 avg loss: 0.0019514178906990167\n",
      "epoch: 400 avg loss: 0.00246861125216176\n",
      "epoch: 500 avg loss: 0.0012846312448872548\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D9664AB9B0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0010405784543380786\n",
      "epoch: 200 avg loss: 0.0014816817908324179\n",
      "epoch: 300 avg loss: 0.0023442837899844897\n",
      "epoch: 400 avg loss: 0.0009609683272030548\n",
      "epoch: 500 avg loss: 0.0015383213181178832\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D966BCDAC8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.001592339338936941\n",
      "epoch: 200 avg loss: 0.0012458410967251677\n",
      "epoch: 300 avg loss: 0.0009068375840893408\n",
      "epoch: 400 avg loss: 0.0023869096643657508\n",
      "epoch: 500 avg loss: 0.0005451865697403647\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D96F9FA080>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.003060313926605312\n",
      "epoch: 200 avg loss: 0.00283868049346462\n",
      "epoch: 300 avg loss: 0.0016862457123352215\n",
      "epoch: 400 avg loss: 0.0017121834624910996\n",
      "epoch: 500 avg loss: 0.0006625499545646706\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D97019BC50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.003938868634577375\n",
      "epoch: 200 avg loss: 0.0011999973656585145\n",
      "epoch: 300 avg loss: 0.0009384121267909282\n",
      "epoch: 400 avg loss: 0.0010367705924038187\n",
      "epoch: 500 avg loss: 0.027210399230458086\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D970865898>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0024019436639623323\n",
      "epoch: 200 avg loss: 0.003360099523744889\n",
      "epoch: 300 avg loss: 0.0008309819236034711\n",
      "epoch: 400 avg loss: 0.0009521418000268137\n",
      "epoch: 500 avg loss: 0.0005686415109889104\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D97FAB9AC8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0025978915587792904\n",
      "epoch: 200 avg loss: 0.0019611188563909594\n",
      "epoch: 300 avg loss: 0.005959643947502819\n",
      "epoch: 400 avg loss: 0.0020721237539730376\n",
      "epoch: 500 avg loss: 0.0017262131443299279\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D906C40080>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0026243617930480183\n",
      "epoch: 200 avg loss: 0.0024864283171298573\n",
      "epoch: 300 avg loss: 0.0020609994944128344\n",
      "epoch: 400 avg loss: 0.00046238735060461823\n",
      "epoch: 500 avg loss: 0.0004204753040539799\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D908306278>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0016036353672940382\n",
      "epoch: 200 avg loss: 0.002436461837772329\n",
      "epoch: 300 avg loss: 0.00110978921216692\n",
      "epoch: 400 avg loss: 0.001326843008619586\n",
      "epoch: 500 avg loss: 0.0009823644579323866\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D908A2D1D0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0042701408758722735\n",
      "epoch: 200 avg loss: 0.0053732321309661\n",
      "epoch: 300 avg loss: 0.003253559307746687\n",
      "epoch: 400 avg loss: 0.0015577282373621863\n",
      "epoch: 500 avg loss: 0.005928255685016905\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D90AD3BB00>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0033048642263135038\n",
      "epoch: 200 avg loss: 0.0009325137901836761\n",
      "epoch: 300 avg loss: 0.0027411084363273787\n",
      "epoch: 400 avg loss: 0.0009192483714861354\n",
      "epoch: 500 avg loss: 0.0008418400359101055\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D9108BBC18>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0016506214018782454\n",
      "epoch: 200 avg loss: 0.0014195649484080558\n",
      "epoch: 300 avg loss: 0.0007214876865729916\n",
      "epoch: 400 avg loss: 0.0016584452335480723\n",
      "epoch: 500 avg loss: 0.0008008791834359535\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D911FB3278>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.002330040611253169\n",
      "epoch: 200 avg loss: 0.0011085876597606234\n",
      "epoch: 300 avg loss: 0.0009533555015590728\n",
      "epoch: 400 avg loss: 0.0012300225958154585\n",
      "epoch: 500 avg loss: 0.001169663821632352\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D9126D61D0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.003145661467753694\n",
      "epoch: 200 avg loss: 0.0010876760402185794\n",
      "epoch: 300 avg loss: 0.0006977791354553407\n",
      "epoch: 400 avg loss: 0.0018932702342250483\n",
      "epoch: 500 avg loss: 0.0010542019670921083\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D913DC7B00>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.00551561263120292\n",
      "epoch: 200 avg loss: 0.0015415472210550338\n",
      "epoch: 300 avg loss: 0.002333478226271262\n",
      "epoch: 400 avg loss: 0.0019301413960515913\n",
      "epoch: 500 avg loss: 0.0008722521934511238\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D913FEC7B8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.002176416657580136\n",
      "epoch: 200 avg loss: 0.004503822293420518\n",
      "epoch: 300 avg loss: 0.0030025085652596317\n",
      "epoch: 400 avg loss: 0.0016397822686198388\n",
      "epoch: 500 avg loss: 0.001350734085319441\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D90060E978>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.0010094287461374495\n",
      "epoch: 200 avg loss: 0.000890858771239628\n",
      "epoch: 300 avg loss: 0.0008454111771961356\n",
      "epoch: 400 avg loss: 0.0008308555237530719\n",
      "epoch: 500 avg loss: 0.0010320640599514406\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D9043E5DA0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:364: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:365: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: nan\n",
      "epoch: 200 avg loss: nan\n",
      "epoch: 300 avg loss: nan\n",
      "epoch: 400 avg loss: nan\n",
      "epoch: 500 avg loss: nan\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001D904422FD0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Geoffroy\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.004150590332325297\n",
      "epoch: 200 avg loss: 0.003788782672257328\n",
      "epoch: 300 avg loss: 0.0022630513633071373\n",
      "epoch: 400 avg loss: 0.0003111942957107921\n",
      "epoch: 500 avg loss: 0.002542437161058555\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMM_LSTM</th>\n",
       "      <th>ABT_LSTM</th>\n",
       "      <th>ABBV_LSTM</th>\n",
       "      <th>ABMD_LSTM</th>\n",
       "      <th>ACN_LSTM</th>\n",
       "      <th>ATVI_LSTM</th>\n",
       "      <th>ADBE_LSTM</th>\n",
       "      <th>AMD_LSTM</th>\n",
       "      <th>AAP_LSTM</th>\n",
       "      <th>AES_LSTM</th>\n",
       "      <th>...</th>\n",
       "      <th>ANSS_LSTM</th>\n",
       "      <th>ANTM_LSTM</th>\n",
       "      <th>AON_LSTM</th>\n",
       "      <th>AOS_LSTM</th>\n",
       "      <th>APA_LSTM</th>\n",
       "      <th>AIV_LSTM</th>\n",
       "      <th>AAPL_LSTM</th>\n",
       "      <th>AMAT_LSTM</th>\n",
       "      <th>APTV_LSTM</th>\n",
       "      <th>ADM_LSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>4.315480</td>\n",
       "      <td>1.475366</td>\n",
       "      <td>3.338932</td>\n",
       "      <td>5.178884</td>\n",
       "      <td>2.229365</td>\n",
       "      <td>1.978180</td>\n",
       "      <td>2.998095</td>\n",
       "      <td>0.306556</td>\n",
       "      <td>19.451953</td>\n",
       "      <td>0.630900</td>\n",
       "      <td>...</td>\n",
       "      <td>2.310868</td>\n",
       "      <td>4.973227</td>\n",
       "      <td>1.804057</td>\n",
       "      <td>3.259446</td>\n",
       "      <td>2.577447</td>\n",
       "      <td>1.298300</td>\n",
       "      <td>5.065431</td>\n",
       "      <td>1.187498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.256220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>4.582275</td>\n",
       "      <td>1.660137</td>\n",
       "      <td>3.289510</td>\n",
       "      <td>4.801923</td>\n",
       "      <td>2.190262</td>\n",
       "      <td>1.873648</td>\n",
       "      <td>3.533528</td>\n",
       "      <td>0.321808</td>\n",
       "      <td>20.026024</td>\n",
       "      <td>0.587416</td>\n",
       "      <td>...</td>\n",
       "      <td>2.613114</td>\n",
       "      <td>4.823492</td>\n",
       "      <td>1.957515</td>\n",
       "      <td>3.481168</td>\n",
       "      <td>2.479536</td>\n",
       "      <td>1.310285</td>\n",
       "      <td>5.184880</td>\n",
       "      <td>1.176876</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.689938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>4.883265</td>\n",
       "      <td>1.925244</td>\n",
       "      <td>3.156833</td>\n",
       "      <td>4.236933</td>\n",
       "      <td>2.167694</td>\n",
       "      <td>1.807244</td>\n",
       "      <td>4.102010</td>\n",
       "      <td>0.352488</td>\n",
       "      <td>19.286654</td>\n",
       "      <td>0.520876</td>\n",
       "      <td>...</td>\n",
       "      <td>3.064241</td>\n",
       "      <td>4.737156</td>\n",
       "      <td>2.297171</td>\n",
       "      <td>3.650515</td>\n",
       "      <td>2.560338</td>\n",
       "      <td>1.336107</td>\n",
       "      <td>5.260967</td>\n",
       "      <td>1.161684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.905931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>5.219643</td>\n",
       "      <td>2.206390</td>\n",
       "      <td>3.037402</td>\n",
       "      <td>3.234049</td>\n",
       "      <td>2.072338</td>\n",
       "      <td>1.807770</td>\n",
       "      <td>4.878557</td>\n",
       "      <td>0.421332</td>\n",
       "      <td>16.709980</td>\n",
       "      <td>0.447555</td>\n",
       "      <td>...</td>\n",
       "      <td>3.078428</td>\n",
       "      <td>4.628940</td>\n",
       "      <td>2.792172</td>\n",
       "      <td>4.003664</td>\n",
       "      <td>2.610692</td>\n",
       "      <td>1.389194</td>\n",
       "      <td>5.270358</td>\n",
       "      <td>1.144776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.109708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>5.487511</td>\n",
       "      <td>2.587518</td>\n",
       "      <td>2.873133</td>\n",
       "      <td>2.475744</td>\n",
       "      <td>2.162069</td>\n",
       "      <td>1.903919</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.444213</td>\n",
       "      <td>13.889608</td>\n",
       "      <td>0.376120</td>\n",
       "      <td>...</td>\n",
       "      <td>3.262271</td>\n",
       "      <td>4.480877</td>\n",
       "      <td>3.205117</td>\n",
       "      <td>4.315747</td>\n",
       "      <td>2.660851</td>\n",
       "      <td>1.443448</td>\n",
       "      <td>5.351387</td>\n",
       "      <td>1.136698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.247321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     MMM_LSTM  ABT_LSTM  ABBV_LSTM  ABMD_LSTM  ACN_LSTM  ATVI_LSTM  ADBE_LSTM  \\\n",
       "194  4.315480  1.475366   3.338932   5.178884  2.229365   1.978180   2.998095   \n",
       "195  4.582275  1.660137   3.289510   4.801923  2.190262   1.873648   3.533528   \n",
       "196  4.883265  1.925244   3.156833   4.236933  2.167694   1.807244   4.102010   \n",
       "197  5.219643  2.206390   3.037402   3.234049  2.072338   1.807770   4.878557   \n",
       "198  5.487511  2.587518   2.873133   2.475744  2.162069   1.903919   5.600000   \n",
       "\n",
       "     AMD_LSTM   AAP_LSTM  AES_LSTM  ...  ANSS_LSTM  ANTM_LSTM  AON_LSTM  \\\n",
       "194  0.306556  19.451953  0.630900  ...   2.310868   4.973227  1.804057   \n",
       "195  0.321808  20.026024  0.587416  ...   2.613114   4.823492  1.957515   \n",
       "196  0.352488  19.286654  0.520876  ...   3.064241   4.737156  2.297171   \n",
       "197  0.421332  16.709980  0.447555  ...   3.078428   4.628940  2.792172   \n",
       "198  0.444213  13.889608  0.376120  ...   3.262271   4.480877  3.205117   \n",
       "\n",
       "     AOS_LSTM  APA_LSTM  AIV_LSTM  AAPL_LSTM  AMAT_LSTM  APTV_LSTM  ADM_LSTM  \n",
       "194  3.259446  2.577447  1.298300   5.065431   1.187498        NaN  4.256220  \n",
       "195  3.481168  2.479536  1.310285   5.184880   1.176876        NaN  3.689938  \n",
       "196  3.650515  2.560338  1.336107   5.260967   1.161684        NaN  2.905931  \n",
       "197  4.003664  2.610692  1.389194   5.270358   1.144776        NaN  2.109708  \n",
       "198  4.315747  2.660851  1.443448   5.351387   1.136698        NaN  2.247321  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()\n",
    "\n",
    "df_LSTM=pd.DataFrame()\n",
    "for l in logo_all[:]:\n",
    "    df_log, date_ori2 =prediction_lstm(df[[\"Date\",l]],date_ori)\n",
    "    df_LSTM[l+'_LSTM']=pd.DataFrame(df_log)[0].tolist()\n",
    "    \n",
    "df_LSTM.tail() \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()\n",
    "#df_log5, date_ori5 =prediction_lstm(df[[\"Date\",'MMM']],date_ori)\n",
    "#df_LSTM['MMM'+'_LSTM']=pd.DataFrame(df_log5)[0].tolist()\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMM_LSTM</th>\n",
       "      <th>ABT_LSTM</th>\n",
       "      <th>ABBV_LSTM</th>\n",
       "      <th>ABMD_LSTM</th>\n",
       "      <th>ACN_LSTM</th>\n",
       "      <th>ATVI_LSTM</th>\n",
       "      <th>ADBE_LSTM</th>\n",
       "      <th>AMD_LSTM</th>\n",
       "      <th>AAP_LSTM</th>\n",
       "      <th>AES_LSTM</th>\n",
       "      <th>...</th>\n",
       "      <th>ANSS_LSTM</th>\n",
       "      <th>ANTM_LSTM</th>\n",
       "      <th>AON_LSTM</th>\n",
       "      <th>AOS_LSTM</th>\n",
       "      <th>APA_LSTM</th>\n",
       "      <th>AIV_LSTM</th>\n",
       "      <th>AAPL_LSTM</th>\n",
       "      <th>AMAT_LSTM</th>\n",
       "      <th>APTV_LSTM</th>\n",
       "      <th>ADM_LSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.746678</td>\n",
       "      <td>1.263829</td>\n",
       "      <td>2.977300</td>\n",
       "      <td>10.950496</td>\n",
       "      <td>2.121233</td>\n",
       "      <td>1.582992</td>\n",
       "      <td>3.045472</td>\n",
       "      <td>0.236905</td>\n",
       "      <td>4.414233</td>\n",
       "      <td>0.454450</td>\n",
       "      <td>...</td>\n",
       "      <td>2.496719</td>\n",
       "      <td>8.977930</td>\n",
       "      <td>3.036090</td>\n",
       "      <td>3.588603</td>\n",
       "      <td>3.220512</td>\n",
       "      <td>1.146114</td>\n",
       "      <td>7.914326</td>\n",
       "      <td>0.708415</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.965052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.604462</td>\n",
       "      <td>1.347925</td>\n",
       "      <td>2.837328</td>\n",
       "      <td>11.502473</td>\n",
       "      <td>2.076994</td>\n",
       "      <td>1.385383</td>\n",
       "      <td>3.458163</td>\n",
       "      <td>0.257399</td>\n",
       "      <td>3.069070</td>\n",
       "      <td>0.372114</td>\n",
       "      <td>...</td>\n",
       "      <td>2.639326</td>\n",
       "      <td>8.816739</td>\n",
       "      <td>3.086694</td>\n",
       "      <td>3.499710</td>\n",
       "      <td>3.023498</td>\n",
       "      <td>1.032093</td>\n",
       "      <td>7.419336</td>\n",
       "      <td>0.761793</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.450033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.506720</td>\n",
       "      <td>1.532234</td>\n",
       "      <td>2.690357</td>\n",
       "      <td>11.359133</td>\n",
       "      <td>2.217411</td>\n",
       "      <td>1.456448</td>\n",
       "      <td>3.177202</td>\n",
       "      <td>0.243733</td>\n",
       "      <td>4.770209</td>\n",
       "      <td>0.415398</td>\n",
       "      <td>...</td>\n",
       "      <td>2.791107</td>\n",
       "      <td>8.751988</td>\n",
       "      <td>3.047426</td>\n",
       "      <td>3.315575</td>\n",
       "      <td>3.198543</td>\n",
       "      <td>1.145909</td>\n",
       "      <td>7.389762</td>\n",
       "      <td>0.723384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.536544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.556544</td>\n",
       "      <td>1.487930</td>\n",
       "      <td>2.586582</td>\n",
       "      <td>11.199201</td>\n",
       "      <td>2.315778</td>\n",
       "      <td>1.513432</td>\n",
       "      <td>3.145749</td>\n",
       "      <td>0.245023</td>\n",
       "      <td>4.023721</td>\n",
       "      <td>0.444935</td>\n",
       "      <td>...</td>\n",
       "      <td>2.588377</td>\n",
       "      <td>8.489985</td>\n",
       "      <td>2.980619</td>\n",
       "      <td>3.277602</td>\n",
       "      <td>3.028910</td>\n",
       "      <td>1.106916</td>\n",
       "      <td>7.051504</td>\n",
       "      <td>0.764903</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.332982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.653760</td>\n",
       "      <td>1.448560</td>\n",
       "      <td>2.476370</td>\n",
       "      <td>10.926137</td>\n",
       "      <td>2.452366</td>\n",
       "      <td>1.431906</td>\n",
       "      <td>3.164338</td>\n",
       "      <td>0.239634</td>\n",
       "      <td>4.272506</td>\n",
       "      <td>0.446472</td>\n",
       "      <td>...</td>\n",
       "      <td>2.543484</td>\n",
       "      <td>8.544436</td>\n",
       "      <td>2.982077</td>\n",
       "      <td>3.333642</td>\n",
       "      <td>3.034087</td>\n",
       "      <td>1.189285</td>\n",
       "      <td>6.987285</td>\n",
       "      <td>0.843652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.359546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.789705</td>\n",
       "      <td>1.466732</td>\n",
       "      <td>2.292356</td>\n",
       "      <td>11.097188</td>\n",
       "      <td>2.597506</td>\n",
       "      <td>1.355233</td>\n",
       "      <td>3.199279</td>\n",
       "      <td>0.235208</td>\n",
       "      <td>4.868787</td>\n",
       "      <td>0.459655</td>\n",
       "      <td>...</td>\n",
       "      <td>2.571283</td>\n",
       "      <td>8.408281</td>\n",
       "      <td>2.895457</td>\n",
       "      <td>3.329290</td>\n",
       "      <td>2.981835</td>\n",
       "      <td>1.173093</td>\n",
       "      <td>6.746178</td>\n",
       "      <td>0.838837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.681497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.520596</td>\n",
       "      <td>1.498231</td>\n",
       "      <td>2.372945</td>\n",
       "      <td>10.901910</td>\n",
       "      <td>2.656395</td>\n",
       "      <td>1.284487</td>\n",
       "      <td>3.097516</td>\n",
       "      <td>0.236457</td>\n",
       "      <td>4.157387</td>\n",
       "      <td>0.459785</td>\n",
       "      <td>...</td>\n",
       "      <td>2.600001</td>\n",
       "      <td>8.493305</td>\n",
       "      <td>2.948728</td>\n",
       "      <td>3.217503</td>\n",
       "      <td>3.034510</td>\n",
       "      <td>1.184590</td>\n",
       "      <td>6.607978</td>\n",
       "      <td>0.842367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.482274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.456314</td>\n",
       "      <td>1.454131</td>\n",
       "      <td>2.275495</td>\n",
       "      <td>10.609546</td>\n",
       "      <td>2.834765</td>\n",
       "      <td>1.199835</td>\n",
       "      <td>3.044875</td>\n",
       "      <td>0.236943</td>\n",
       "      <td>4.529382</td>\n",
       "      <td>0.476904</td>\n",
       "      <td>...</td>\n",
       "      <td>2.561957</td>\n",
       "      <td>7.885571</td>\n",
       "      <td>2.927686</td>\n",
       "      <td>3.169986</td>\n",
       "      <td>2.956802</td>\n",
       "      <td>1.174159</td>\n",
       "      <td>6.377526</td>\n",
       "      <td>0.881368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.472192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.532359</td>\n",
       "      <td>1.444410</td>\n",
       "      <td>2.048051</td>\n",
       "      <td>10.361605</td>\n",
       "      <td>2.871272</td>\n",
       "      <td>1.100978</td>\n",
       "      <td>3.038062</td>\n",
       "      <td>0.234150</td>\n",
       "      <td>4.305710</td>\n",
       "      <td>0.490974</td>\n",
       "      <td>...</td>\n",
       "      <td>2.441464</td>\n",
       "      <td>7.776992</td>\n",
       "      <td>2.874181</td>\n",
       "      <td>3.162521</td>\n",
       "      <td>2.992117</td>\n",
       "      <td>1.175147</td>\n",
       "      <td>6.397264</td>\n",
       "      <td>0.898675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.355063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.371970</td>\n",
       "      <td>1.473193</td>\n",
       "      <td>2.097228</td>\n",
       "      <td>9.949480</td>\n",
       "      <td>2.899386</td>\n",
       "      <td>1.052868</td>\n",
       "      <td>2.904028</td>\n",
       "      <td>0.234060</td>\n",
       "      <td>4.820348</td>\n",
       "      <td>0.474304</td>\n",
       "      <td>...</td>\n",
       "      <td>2.348216</td>\n",
       "      <td>7.493142</td>\n",
       "      <td>2.833179</td>\n",
       "      <td>2.988652</td>\n",
       "      <td>2.982576</td>\n",
       "      <td>1.188101</td>\n",
       "      <td>6.143486</td>\n",
       "      <td>0.949476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.534804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.328750</td>\n",
       "      <td>1.463590</td>\n",
       "      <td>2.194467</td>\n",
       "      <td>9.686427</td>\n",
       "      <td>2.924464</td>\n",
       "      <td>0.978868</td>\n",
       "      <td>2.911077</td>\n",
       "      <td>0.231943</td>\n",
       "      <td>4.740590</td>\n",
       "      <td>0.477061</td>\n",
       "      <td>...</td>\n",
       "      <td>2.295630</td>\n",
       "      <td>7.389724</td>\n",
       "      <td>2.831971</td>\n",
       "      <td>2.780073</td>\n",
       "      <td>3.248208</td>\n",
       "      <td>1.195855</td>\n",
       "      <td>5.578852</td>\n",
       "      <td>0.931365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.445584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.173480</td>\n",
       "      <td>1.394488</td>\n",
       "      <td>2.222129</td>\n",
       "      <td>9.138044</td>\n",
       "      <td>3.019649</td>\n",
       "      <td>0.906515</td>\n",
       "      <td>2.911084</td>\n",
       "      <td>0.214102</td>\n",
       "      <td>4.112461</td>\n",
       "      <td>0.456521</td>\n",
       "      <td>...</td>\n",
       "      <td>2.291628</td>\n",
       "      <td>7.365362</td>\n",
       "      <td>2.855068</td>\n",
       "      <td>2.620598</td>\n",
       "      <td>3.228076</td>\n",
       "      <td>1.205950</td>\n",
       "      <td>5.126293</td>\n",
       "      <td>0.932730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.520785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.920112</td>\n",
       "      <td>1.346577</td>\n",
       "      <td>2.230808</td>\n",
       "      <td>8.712054</td>\n",
       "      <td>3.000086</td>\n",
       "      <td>0.992611</td>\n",
       "      <td>2.771694</td>\n",
       "      <td>0.191725</td>\n",
       "      <td>4.694531</td>\n",
       "      <td>0.461096</td>\n",
       "      <td>...</td>\n",
       "      <td>2.283102</td>\n",
       "      <td>7.405556</td>\n",
       "      <td>2.819233</td>\n",
       "      <td>2.547849</td>\n",
       "      <td>3.479354</td>\n",
       "      <td>1.207887</td>\n",
       "      <td>5.023874</td>\n",
       "      <td>0.955587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.223810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.357292</td>\n",
       "      <td>1.339398</td>\n",
       "      <td>1.948196</td>\n",
       "      <td>8.214464</td>\n",
       "      <td>3.053873</td>\n",
       "      <td>0.807891</td>\n",
       "      <td>2.865607</td>\n",
       "      <td>0.203085</td>\n",
       "      <td>4.794912</td>\n",
       "      <td>0.462295</td>\n",
       "      <td>...</td>\n",
       "      <td>2.254327</td>\n",
       "      <td>7.203054</td>\n",
       "      <td>2.827205</td>\n",
       "      <td>2.206318</td>\n",
       "      <td>3.298482</td>\n",
       "      <td>1.213761</td>\n",
       "      <td>4.754395</td>\n",
       "      <td>1.030397</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.477746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.246773</td>\n",
       "      <td>1.342822</td>\n",
       "      <td>2.074416</td>\n",
       "      <td>7.930867</td>\n",
       "      <td>3.076565</td>\n",
       "      <td>0.756071</td>\n",
       "      <td>2.793038</td>\n",
       "      <td>0.199992</td>\n",
       "      <td>4.509519</td>\n",
       "      <td>0.484440</td>\n",
       "      <td>...</td>\n",
       "      <td>2.227126</td>\n",
       "      <td>7.421291</td>\n",
       "      <td>2.785801</td>\n",
       "      <td>1.935241</td>\n",
       "      <td>3.450245</td>\n",
       "      <td>1.192601</td>\n",
       "      <td>4.646911</td>\n",
       "      <td>1.031773</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.415236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.343300</td>\n",
       "      <td>1.356081</td>\n",
       "      <td>2.338900</td>\n",
       "      <td>6.949923</td>\n",
       "      <td>3.089094</td>\n",
       "      <td>0.714931</td>\n",
       "      <td>2.766054</td>\n",
       "      <td>0.208402</td>\n",
       "      <td>4.315370</td>\n",
       "      <td>0.496845</td>\n",
       "      <td>...</td>\n",
       "      <td>2.240983</td>\n",
       "      <td>7.567932</td>\n",
       "      <td>2.704467</td>\n",
       "      <td>1.910227</td>\n",
       "      <td>3.644313</td>\n",
       "      <td>1.159541</td>\n",
       "      <td>4.326182</td>\n",
       "      <td>1.041937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.488241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.273836</td>\n",
       "      <td>1.344707</td>\n",
       "      <td>2.239627</td>\n",
       "      <td>6.325040</td>\n",
       "      <td>3.077646</td>\n",
       "      <td>0.722908</td>\n",
       "      <td>2.807404</td>\n",
       "      <td>0.231055</td>\n",
       "      <td>4.588702</td>\n",
       "      <td>0.519169</td>\n",
       "      <td>...</td>\n",
       "      <td>2.269269</td>\n",
       "      <td>7.267528</td>\n",
       "      <td>2.678356</td>\n",
       "      <td>1.802348</td>\n",
       "      <td>3.563947</td>\n",
       "      <td>1.121177</td>\n",
       "      <td>4.115644</td>\n",
       "      <td>1.043621</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.482004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.311069</td>\n",
       "      <td>1.293249</td>\n",
       "      <td>2.279090</td>\n",
       "      <td>6.009716</td>\n",
       "      <td>3.084542</td>\n",
       "      <td>0.581665</td>\n",
       "      <td>2.709841</td>\n",
       "      <td>0.238287</td>\n",
       "      <td>4.820841</td>\n",
       "      <td>0.520136</td>\n",
       "      <td>...</td>\n",
       "      <td>2.242296</td>\n",
       "      <td>7.194514</td>\n",
       "      <td>2.550920</td>\n",
       "      <td>1.879237</td>\n",
       "      <td>3.615629</td>\n",
       "      <td>1.065911</td>\n",
       "      <td>3.399671</td>\n",
       "      <td>1.065640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.464902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.518391</td>\n",
       "      <td>1.202951</td>\n",
       "      <td>2.687167</td>\n",
       "      <td>6.176762</td>\n",
       "      <td>2.899903</td>\n",
       "      <td>0.628997</td>\n",
       "      <td>2.601374</td>\n",
       "      <td>0.243938</td>\n",
       "      <td>4.788873</td>\n",
       "      <td>0.527337</td>\n",
       "      <td>...</td>\n",
       "      <td>2.088383</td>\n",
       "      <td>6.904310</td>\n",
       "      <td>2.367507</td>\n",
       "      <td>1.654096</td>\n",
       "      <td>3.543355</td>\n",
       "      <td>1.023570</td>\n",
       "      <td>3.763044</td>\n",
       "      <td>1.085537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.326605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.728711</td>\n",
       "      <td>1.248739</td>\n",
       "      <td>2.528518</td>\n",
       "      <td>6.440399</td>\n",
       "      <td>2.839310</td>\n",
       "      <td>0.528580</td>\n",
       "      <td>2.676202</td>\n",
       "      <td>0.256797</td>\n",
       "      <td>3.905598</td>\n",
       "      <td>0.521383</td>\n",
       "      <td>...</td>\n",
       "      <td>2.045553</td>\n",
       "      <td>6.689503</td>\n",
       "      <td>2.175531</td>\n",
       "      <td>1.789558</td>\n",
       "      <td>3.515821</td>\n",
       "      <td>1.007385</td>\n",
       "      <td>3.882660</td>\n",
       "      <td>1.231544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.705204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.983467</td>\n",
       "      <td>1.170245</td>\n",
       "      <td>2.745014</td>\n",
       "      <td>5.487902</td>\n",
       "      <td>2.678045</td>\n",
       "      <td>0.450003</td>\n",
       "      <td>2.357194</td>\n",
       "      <td>0.260133</td>\n",
       "      <td>4.313820</td>\n",
       "      <td>0.538857</td>\n",
       "      <td>...</td>\n",
       "      <td>1.799425</td>\n",
       "      <td>6.441620</td>\n",
       "      <td>1.939007</td>\n",
       "      <td>1.347219</td>\n",
       "      <td>3.444860</td>\n",
       "      <td>1.034110</td>\n",
       "      <td>3.347233</td>\n",
       "      <td>1.349575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.417349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.039682</td>\n",
       "      <td>1.137266</td>\n",
       "      <td>2.888206</td>\n",
       "      <td>4.848337</td>\n",
       "      <td>2.500256</td>\n",
       "      <td>0.415078</td>\n",
       "      <td>2.228842</td>\n",
       "      <td>0.275046</td>\n",
       "      <td>4.396197</td>\n",
       "      <td>0.557376</td>\n",
       "      <td>...</td>\n",
       "      <td>1.706838</td>\n",
       "      <td>6.149405</td>\n",
       "      <td>1.857300</td>\n",
       "      <td>1.277434</td>\n",
       "      <td>3.583659</td>\n",
       "      <td>1.030087</td>\n",
       "      <td>3.240056</td>\n",
       "      <td>1.373117</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.549303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.161940</td>\n",
       "      <td>1.071580</td>\n",
       "      <td>2.941781</td>\n",
       "      <td>5.295352</td>\n",
       "      <td>2.586976</td>\n",
       "      <td>0.305217</td>\n",
       "      <td>2.124664</td>\n",
       "      <td>0.287846</td>\n",
       "      <td>4.595329</td>\n",
       "      <td>0.563305</td>\n",
       "      <td>...</td>\n",
       "      <td>1.625999</td>\n",
       "      <td>5.854003</td>\n",
       "      <td>1.783817</td>\n",
       "      <td>1.377017</td>\n",
       "      <td>3.498826</td>\n",
       "      <td>0.998623</td>\n",
       "      <td>2.920755</td>\n",
       "      <td>1.412284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.502515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.374002</td>\n",
       "      <td>1.030071</td>\n",
       "      <td>2.799276</td>\n",
       "      <td>4.812072</td>\n",
       "      <td>2.630831</td>\n",
       "      <td>0.308008</td>\n",
       "      <td>2.222906</td>\n",
       "      <td>0.292874</td>\n",
       "      <td>4.997905</td>\n",
       "      <td>0.569763</td>\n",
       "      <td>...</td>\n",
       "      <td>1.521136</td>\n",
       "      <td>5.740407</td>\n",
       "      <td>1.411998</td>\n",
       "      <td>1.302062</td>\n",
       "      <td>3.595644</td>\n",
       "      <td>1.017976</td>\n",
       "      <td>2.902311</td>\n",
       "      <td>1.529234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.687156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.363330</td>\n",
       "      <td>1.119236</td>\n",
       "      <td>2.900210</td>\n",
       "      <td>5.062479</td>\n",
       "      <td>2.547870</td>\n",
       "      <td>0.316480</td>\n",
       "      <td>2.230299</td>\n",
       "      <td>0.275338</td>\n",
       "      <td>4.359016</td>\n",
       "      <td>0.587059</td>\n",
       "      <td>...</td>\n",
       "      <td>1.644097</td>\n",
       "      <td>5.546580</td>\n",
       "      <td>1.519546</td>\n",
       "      <td>1.290197</td>\n",
       "      <td>3.520003</td>\n",
       "      <td>0.896143</td>\n",
       "      <td>2.978411</td>\n",
       "      <td>1.574244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.649101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.672615</td>\n",
       "      <td>1.048164</td>\n",
       "      <td>2.877817</td>\n",
       "      <td>4.701971</td>\n",
       "      <td>2.415934</td>\n",
       "      <td>0.373778</td>\n",
       "      <td>2.094470</td>\n",
       "      <td>0.291582</td>\n",
       "      <td>4.073099</td>\n",
       "      <td>0.578791</td>\n",
       "      <td>...</td>\n",
       "      <td>1.483976</td>\n",
       "      <td>5.254621</td>\n",
       "      <td>1.573419</td>\n",
       "      <td>1.461269</td>\n",
       "      <td>3.514529</td>\n",
       "      <td>0.940581</td>\n",
       "      <td>2.869073</td>\n",
       "      <td>1.618892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.852723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.797816</td>\n",
       "      <td>0.898898</td>\n",
       "      <td>2.790826</td>\n",
       "      <td>5.299451</td>\n",
       "      <td>2.446560</td>\n",
       "      <td>0.242471</td>\n",
       "      <td>1.782888</td>\n",
       "      <td>0.287581</td>\n",
       "      <td>3.857609</td>\n",
       "      <td>0.569784</td>\n",
       "      <td>...</td>\n",
       "      <td>1.393417</td>\n",
       "      <td>4.940042</td>\n",
       "      <td>1.449605</td>\n",
       "      <td>1.433980</td>\n",
       "      <td>3.497093</td>\n",
       "      <td>0.964321</td>\n",
       "      <td>2.697359</td>\n",
       "      <td>1.701124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.781207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.776825</td>\n",
       "      <td>0.787825</td>\n",
       "      <td>2.962956</td>\n",
       "      <td>5.058153</td>\n",
       "      <td>2.537338</td>\n",
       "      <td>0.287806</td>\n",
       "      <td>1.968989</td>\n",
       "      <td>0.290458</td>\n",
       "      <td>2.805261</td>\n",
       "      <td>0.579130</td>\n",
       "      <td>...</td>\n",
       "      <td>1.483398</td>\n",
       "      <td>4.720181</td>\n",
       "      <td>1.617377</td>\n",
       "      <td>1.257527</td>\n",
       "      <td>3.552906</td>\n",
       "      <td>0.940872</td>\n",
       "      <td>2.659901</td>\n",
       "      <td>1.774058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.810649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.732041</td>\n",
       "      <td>0.803765</td>\n",
       "      <td>2.944781</td>\n",
       "      <td>5.164745</td>\n",
       "      <td>2.452312</td>\n",
       "      <td>0.553298</td>\n",
       "      <td>1.870477</td>\n",
       "      <td>0.318674</td>\n",
       "      <td>3.333326</td>\n",
       "      <td>0.583295</td>\n",
       "      <td>...</td>\n",
       "      <td>1.427096</td>\n",
       "      <td>4.826407</td>\n",
       "      <td>1.433380</td>\n",
       "      <td>1.391799</td>\n",
       "      <td>3.588790</td>\n",
       "      <td>1.006390</td>\n",
       "      <td>2.593900</td>\n",
       "      <td>1.757291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.972307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.740077</td>\n",
       "      <td>0.946909</td>\n",
       "      <td>2.860651</td>\n",
       "      <td>4.506537</td>\n",
       "      <td>2.475805</td>\n",
       "      <td>0.476938</td>\n",
       "      <td>1.973807</td>\n",
       "      <td>0.291063</td>\n",
       "      <td>3.237543</td>\n",
       "      <td>0.574267</td>\n",
       "      <td>...</td>\n",
       "      <td>1.473566</td>\n",
       "      <td>4.775004</td>\n",
       "      <td>1.481441</td>\n",
       "      <td>1.463359</td>\n",
       "      <td>3.586575</td>\n",
       "      <td>0.960226</td>\n",
       "      <td>2.949771</td>\n",
       "      <td>1.789630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.053880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>7.716015</td>\n",
       "      <td>2.004672</td>\n",
       "      <td>3.552657</td>\n",
       "      <td>8.714144</td>\n",
       "      <td>4.389483</td>\n",
       "      <td>2.639385</td>\n",
       "      <td>4.409557</td>\n",
       "      <td>0.185497</td>\n",
       "      <td>12.872873</td>\n",
       "      <td>0.545758</td>\n",
       "      <td>...</td>\n",
       "      <td>2.678195</td>\n",
       "      <td>5.998573</td>\n",
       "      <td>2.002035</td>\n",
       "      <td>5.070417</td>\n",
       "      <td>4.261664</td>\n",
       "      <td>1.659961</td>\n",
       "      <td>3.830608</td>\n",
       "      <td>1.148936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.106014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>7.346168</td>\n",
       "      <td>1.899821</td>\n",
       "      <td>3.516581</td>\n",
       "      <td>8.940817</td>\n",
       "      <td>4.202414</td>\n",
       "      <td>2.665159</td>\n",
       "      <td>4.517550</td>\n",
       "      <td>0.195057</td>\n",
       "      <td>12.868512</td>\n",
       "      <td>0.531984</td>\n",
       "      <td>...</td>\n",
       "      <td>2.657952</td>\n",
       "      <td>6.107925</td>\n",
       "      <td>2.020830</td>\n",
       "      <td>4.765053</td>\n",
       "      <td>4.239940</td>\n",
       "      <td>1.585587</td>\n",
       "      <td>3.610066</td>\n",
       "      <td>1.194368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.050259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>7.462346</td>\n",
       "      <td>1.967792</td>\n",
       "      <td>3.551240</td>\n",
       "      <td>8.801637</td>\n",
       "      <td>4.100710</td>\n",
       "      <td>2.589188</td>\n",
       "      <td>4.628248</td>\n",
       "      <td>0.199579</td>\n",
       "      <td>13.359957</td>\n",
       "      <td>0.535028</td>\n",
       "      <td>...</td>\n",
       "      <td>2.698160</td>\n",
       "      <td>6.166174</td>\n",
       "      <td>1.980290</td>\n",
       "      <td>4.862331</td>\n",
       "      <td>4.277868</td>\n",
       "      <td>1.508472</td>\n",
       "      <td>3.625078</td>\n",
       "      <td>1.267123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.291612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>7.365163</td>\n",
       "      <td>2.017829</td>\n",
       "      <td>3.478335</td>\n",
       "      <td>8.954115</td>\n",
       "      <td>4.252169</td>\n",
       "      <td>2.531693</td>\n",
       "      <td>4.447756</td>\n",
       "      <td>0.201665</td>\n",
       "      <td>13.156663</td>\n",
       "      <td>0.537011</td>\n",
       "      <td>...</td>\n",
       "      <td>2.651728</td>\n",
       "      <td>6.101252</td>\n",
       "      <td>2.025902</td>\n",
       "      <td>4.961339</td>\n",
       "      <td>4.215969</td>\n",
       "      <td>1.535045</td>\n",
       "      <td>3.706531</td>\n",
       "      <td>1.285716</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.329777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>6.985539</td>\n",
       "      <td>2.012576</td>\n",
       "      <td>3.468397</td>\n",
       "      <td>8.962796</td>\n",
       "      <td>4.083341</td>\n",
       "      <td>2.465862</td>\n",
       "      <td>4.211867</td>\n",
       "      <td>0.195546</td>\n",
       "      <td>13.279577</td>\n",
       "      <td>0.556876</td>\n",
       "      <td>...</td>\n",
       "      <td>2.676324</td>\n",
       "      <td>6.263401</td>\n",
       "      <td>2.051124</td>\n",
       "      <td>4.862581</td>\n",
       "      <td>4.156904</td>\n",
       "      <td>1.457688</td>\n",
       "      <td>3.553498</td>\n",
       "      <td>1.284250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.507134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>7.079862</td>\n",
       "      <td>2.072668</td>\n",
       "      <td>3.597387</td>\n",
       "      <td>8.756420</td>\n",
       "      <td>4.013752</td>\n",
       "      <td>2.534108</td>\n",
       "      <td>4.300268</td>\n",
       "      <td>0.196225</td>\n",
       "      <td>14.549615</td>\n",
       "      <td>0.558480</td>\n",
       "      <td>...</td>\n",
       "      <td>2.637138</td>\n",
       "      <td>6.296924</td>\n",
       "      <td>2.215502</td>\n",
       "      <td>5.014201</td>\n",
       "      <td>4.115918</td>\n",
       "      <td>1.420992</td>\n",
       "      <td>3.686825</td>\n",
       "      <td>1.340262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.497783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>7.325026</td>\n",
       "      <td>1.982349</td>\n",
       "      <td>3.483316</td>\n",
       "      <td>8.681897</td>\n",
       "      <td>4.068781</td>\n",
       "      <td>2.548849</td>\n",
       "      <td>4.104004</td>\n",
       "      <td>0.213708</td>\n",
       "      <td>15.649204</td>\n",
       "      <td>0.571184</td>\n",
       "      <td>...</td>\n",
       "      <td>2.618462</td>\n",
       "      <td>6.238167</td>\n",
       "      <td>2.267833</td>\n",
       "      <td>4.837426</td>\n",
       "      <td>4.060811</td>\n",
       "      <td>1.407787</td>\n",
       "      <td>3.613768</td>\n",
       "      <td>1.365421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.405929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>7.146482</td>\n",
       "      <td>2.077264</td>\n",
       "      <td>3.492620</td>\n",
       "      <td>8.400241</td>\n",
       "      <td>4.079584</td>\n",
       "      <td>2.463905</td>\n",
       "      <td>3.987910</td>\n",
       "      <td>0.206864</td>\n",
       "      <td>15.982629</td>\n",
       "      <td>0.566553</td>\n",
       "      <td>...</td>\n",
       "      <td>2.715004</td>\n",
       "      <td>6.213355</td>\n",
       "      <td>2.188086</td>\n",
       "      <td>5.093752</td>\n",
       "      <td>3.926975</td>\n",
       "      <td>1.404183</td>\n",
       "      <td>3.541342</td>\n",
       "      <td>1.409262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.583360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>7.037849</td>\n",
       "      <td>2.097354</td>\n",
       "      <td>3.460556</td>\n",
       "      <td>7.987897</td>\n",
       "      <td>3.837708</td>\n",
       "      <td>2.446741</td>\n",
       "      <td>3.849759</td>\n",
       "      <td>0.208366</td>\n",
       "      <td>15.815984</td>\n",
       "      <td>0.576861</td>\n",
       "      <td>...</td>\n",
       "      <td>2.678911</td>\n",
       "      <td>6.149334</td>\n",
       "      <td>2.377474</td>\n",
       "      <td>4.718745</td>\n",
       "      <td>3.965750</td>\n",
       "      <td>1.353355</td>\n",
       "      <td>3.496858</td>\n",
       "      <td>1.432676</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.863780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>6.969994</td>\n",
       "      <td>2.063510</td>\n",
       "      <td>3.453405</td>\n",
       "      <td>8.373335</td>\n",
       "      <td>3.906883</td>\n",
       "      <td>2.481504</td>\n",
       "      <td>3.784415</td>\n",
       "      <td>0.224124</td>\n",
       "      <td>16.492923</td>\n",
       "      <td>0.566328</td>\n",
       "      <td>...</td>\n",
       "      <td>2.638995</td>\n",
       "      <td>5.815475</td>\n",
       "      <td>2.304302</td>\n",
       "      <td>5.125374</td>\n",
       "      <td>3.967440</td>\n",
       "      <td>1.357178</td>\n",
       "      <td>3.636503</td>\n",
       "      <td>1.446568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.805008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>6.946720</td>\n",
       "      <td>2.055064</td>\n",
       "      <td>3.510028</td>\n",
       "      <td>7.313595</td>\n",
       "      <td>3.807618</td>\n",
       "      <td>2.505655</td>\n",
       "      <td>3.595908</td>\n",
       "      <td>0.218806</td>\n",
       "      <td>17.219788</td>\n",
       "      <td>0.583090</td>\n",
       "      <td>...</td>\n",
       "      <td>2.611610</td>\n",
       "      <td>5.582200</td>\n",
       "      <td>2.337840</td>\n",
       "      <td>4.870176</td>\n",
       "      <td>3.961177</td>\n",
       "      <td>1.341660</td>\n",
       "      <td>3.824827</td>\n",
       "      <td>1.449295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.915082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>6.681860</td>\n",
       "      <td>2.094994</td>\n",
       "      <td>3.458711</td>\n",
       "      <td>7.422924</td>\n",
       "      <td>3.675675</td>\n",
       "      <td>2.505776</td>\n",
       "      <td>3.726813</td>\n",
       "      <td>0.226024</td>\n",
       "      <td>17.706643</td>\n",
       "      <td>0.610506</td>\n",
       "      <td>...</td>\n",
       "      <td>2.658028</td>\n",
       "      <td>5.536940</td>\n",
       "      <td>2.248043</td>\n",
       "      <td>4.776369</td>\n",
       "      <td>3.832967</td>\n",
       "      <td>1.293183</td>\n",
       "      <td>3.672155</td>\n",
       "      <td>1.434632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.107077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>6.331856</td>\n",
       "      <td>2.053636</td>\n",
       "      <td>3.475015</td>\n",
       "      <td>6.712495</td>\n",
       "      <td>3.541226</td>\n",
       "      <td>2.620948</td>\n",
       "      <td>3.721153</td>\n",
       "      <td>0.225081</td>\n",
       "      <td>17.305273</td>\n",
       "      <td>0.608555</td>\n",
       "      <td>...</td>\n",
       "      <td>2.711180</td>\n",
       "      <td>5.360961</td>\n",
       "      <td>2.370515</td>\n",
       "      <td>4.934159</td>\n",
       "      <td>3.757970</td>\n",
       "      <td>1.286796</td>\n",
       "      <td>3.814636</td>\n",
       "      <td>1.445718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.245030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>6.281549</td>\n",
       "      <td>2.137906</td>\n",
       "      <td>3.465244</td>\n",
       "      <td>6.316717</td>\n",
       "      <td>3.526549</td>\n",
       "      <td>2.566958</td>\n",
       "      <td>3.965447</td>\n",
       "      <td>0.227003</td>\n",
       "      <td>18.398152</td>\n",
       "      <td>0.610304</td>\n",
       "      <td>...</td>\n",
       "      <td>2.696485</td>\n",
       "      <td>5.139799</td>\n",
       "      <td>2.308639</td>\n",
       "      <td>5.025581</td>\n",
       "      <td>3.653267</td>\n",
       "      <td>1.304969</td>\n",
       "      <td>3.686934</td>\n",
       "      <td>1.414441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.175756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>5.736904</td>\n",
       "      <td>2.113547</td>\n",
       "      <td>3.434884</td>\n",
       "      <td>5.936811</td>\n",
       "      <td>3.351282</td>\n",
       "      <td>2.504764</td>\n",
       "      <td>3.900218</td>\n",
       "      <td>0.228450</td>\n",
       "      <td>17.343442</td>\n",
       "      <td>0.625456</td>\n",
       "      <td>...</td>\n",
       "      <td>2.693791</td>\n",
       "      <td>4.964303</td>\n",
       "      <td>2.155359</td>\n",
       "      <td>4.723551</td>\n",
       "      <td>3.335475</td>\n",
       "      <td>1.294060</td>\n",
       "      <td>3.918112</td>\n",
       "      <td>1.387179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.172163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>5.457012</td>\n",
       "      <td>2.023819</td>\n",
       "      <td>3.432133</td>\n",
       "      <td>5.739915</td>\n",
       "      <td>3.260415</td>\n",
       "      <td>2.466964</td>\n",
       "      <td>3.641037</td>\n",
       "      <td>0.229165</td>\n",
       "      <td>18.354647</td>\n",
       "      <td>0.628410</td>\n",
       "      <td>...</td>\n",
       "      <td>2.699251</td>\n",
       "      <td>4.689444</td>\n",
       "      <td>2.217362</td>\n",
       "      <td>4.391881</td>\n",
       "      <td>3.283866</td>\n",
       "      <td>1.243571</td>\n",
       "      <td>4.049317</td>\n",
       "      <td>1.381131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.464393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>5.260221</td>\n",
       "      <td>2.004555</td>\n",
       "      <td>3.484759</td>\n",
       "      <td>5.852886</td>\n",
       "      <td>3.036194</td>\n",
       "      <td>2.537002</td>\n",
       "      <td>3.881792</td>\n",
       "      <td>0.223442</td>\n",
       "      <td>18.632970</td>\n",
       "      <td>0.644550</td>\n",
       "      <td>...</td>\n",
       "      <td>2.664533</td>\n",
       "      <td>4.721077</td>\n",
       "      <td>2.160532</td>\n",
       "      <td>4.626076</td>\n",
       "      <td>3.215857</td>\n",
       "      <td>1.240915</td>\n",
       "      <td>4.155048</td>\n",
       "      <td>1.405677</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.531690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>5.099674</td>\n",
       "      <td>1.878920</td>\n",
       "      <td>3.382051</td>\n",
       "      <td>5.663213</td>\n",
       "      <td>2.734810</td>\n",
       "      <td>2.474914</td>\n",
       "      <td>3.490659</td>\n",
       "      <td>0.241492</td>\n",
       "      <td>19.938052</td>\n",
       "      <td>0.648209</td>\n",
       "      <td>...</td>\n",
       "      <td>2.591970</td>\n",
       "      <td>4.950129</td>\n",
       "      <td>1.914180</td>\n",
       "      <td>4.553236</td>\n",
       "      <td>2.958487</td>\n",
       "      <td>1.181020</td>\n",
       "      <td>4.373277</td>\n",
       "      <td>1.354341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.557642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>4.844844</td>\n",
       "      <td>1.786943</td>\n",
       "      <td>3.432712</td>\n",
       "      <td>5.829678</td>\n",
       "      <td>2.504832</td>\n",
       "      <td>2.516272</td>\n",
       "      <td>3.388572</td>\n",
       "      <td>0.240558</td>\n",
       "      <td>19.193150</td>\n",
       "      <td>0.644076</td>\n",
       "      <td>...</td>\n",
       "      <td>2.505128</td>\n",
       "      <td>4.994822</td>\n",
       "      <td>1.932842</td>\n",
       "      <td>4.190211</td>\n",
       "      <td>2.759309</td>\n",
       "      <td>1.183408</td>\n",
       "      <td>4.294200</td>\n",
       "      <td>1.310246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.307787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>4.709681</td>\n",
       "      <td>1.802395</td>\n",
       "      <td>3.304519</td>\n",
       "      <td>6.026285</td>\n",
       "      <td>2.347301</td>\n",
       "      <td>2.342018</td>\n",
       "      <td>3.436087</td>\n",
       "      <td>0.258445</td>\n",
       "      <td>18.816818</td>\n",
       "      <td>0.632181</td>\n",
       "      <td>...</td>\n",
       "      <td>2.439698</td>\n",
       "      <td>4.920035</td>\n",
       "      <td>1.792295</td>\n",
       "      <td>4.057402</td>\n",
       "      <td>2.647423</td>\n",
       "      <td>1.169968</td>\n",
       "      <td>4.398555</td>\n",
       "      <td>1.315898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.329351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>4.367590</td>\n",
       "      <td>1.719529</td>\n",
       "      <td>3.290607</td>\n",
       "      <td>5.892690</td>\n",
       "      <td>2.331561</td>\n",
       "      <td>2.293547</td>\n",
       "      <td>3.467786</td>\n",
       "      <td>0.274186</td>\n",
       "      <td>20.718654</td>\n",
       "      <td>0.649987</td>\n",
       "      <td>...</td>\n",
       "      <td>2.339554</td>\n",
       "      <td>4.933063</td>\n",
       "      <td>1.717406</td>\n",
       "      <td>3.701914</td>\n",
       "      <td>2.602858</td>\n",
       "      <td>1.213056</td>\n",
       "      <td>4.361246</td>\n",
       "      <td>1.266175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.472666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>4.285939</td>\n",
       "      <td>1.687209</td>\n",
       "      <td>3.349431</td>\n",
       "      <td>5.932576</td>\n",
       "      <td>2.126344</td>\n",
       "      <td>2.237429</td>\n",
       "      <td>3.461668</td>\n",
       "      <td>0.290330</td>\n",
       "      <td>20.005320</td>\n",
       "      <td>0.646198</td>\n",
       "      <td>...</td>\n",
       "      <td>2.342745</td>\n",
       "      <td>5.002521</td>\n",
       "      <td>1.846644</td>\n",
       "      <td>3.487907</td>\n",
       "      <td>2.625894</td>\n",
       "      <td>1.244063</td>\n",
       "      <td>4.426616</td>\n",
       "      <td>1.272437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.311843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>4.234381</td>\n",
       "      <td>1.683043</td>\n",
       "      <td>3.339738</td>\n",
       "      <td>5.688180</td>\n",
       "      <td>2.022375</td>\n",
       "      <td>2.203481</td>\n",
       "      <td>3.488759</td>\n",
       "      <td>0.306321</td>\n",
       "      <td>19.833711</td>\n",
       "      <td>0.650585</td>\n",
       "      <td>...</td>\n",
       "      <td>2.314530</td>\n",
       "      <td>4.947623</td>\n",
       "      <td>1.715738</td>\n",
       "      <td>3.418020</td>\n",
       "      <td>2.370275</td>\n",
       "      <td>1.275843</td>\n",
       "      <td>4.481264</td>\n",
       "      <td>1.240368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.379284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>4.130336</td>\n",
       "      <td>1.539781</td>\n",
       "      <td>3.340353</td>\n",
       "      <td>5.775073</td>\n",
       "      <td>2.034177</td>\n",
       "      <td>2.145543</td>\n",
       "      <td>3.480060</td>\n",
       "      <td>0.319380</td>\n",
       "      <td>19.461400</td>\n",
       "      <td>0.638865</td>\n",
       "      <td>...</td>\n",
       "      <td>2.269999</td>\n",
       "      <td>4.959281</td>\n",
       "      <td>1.745817</td>\n",
       "      <td>3.389740</td>\n",
       "      <td>2.526142</td>\n",
       "      <td>1.257741</td>\n",
       "      <td>4.787381</td>\n",
       "      <td>1.223807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.024019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>4.166007</td>\n",
       "      <td>1.513977</td>\n",
       "      <td>3.318076</td>\n",
       "      <td>5.826199</td>\n",
       "      <td>2.050064</td>\n",
       "      <td>2.059565</td>\n",
       "      <td>2.787339</td>\n",
       "      <td>0.322474</td>\n",
       "      <td>18.386773</td>\n",
       "      <td>0.644650</td>\n",
       "      <td>...</td>\n",
       "      <td>2.277900</td>\n",
       "      <td>4.830219</td>\n",
       "      <td>1.915125</td>\n",
       "      <td>3.172297</td>\n",
       "      <td>2.577814</td>\n",
       "      <td>1.269659</td>\n",
       "      <td>4.867476</td>\n",
       "      <td>1.222636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.220589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>4.315480</td>\n",
       "      <td>1.475366</td>\n",
       "      <td>3.338932</td>\n",
       "      <td>5.178884</td>\n",
       "      <td>2.229365</td>\n",
       "      <td>1.978180</td>\n",
       "      <td>2.998095</td>\n",
       "      <td>0.306556</td>\n",
       "      <td>19.451953</td>\n",
       "      <td>0.630900</td>\n",
       "      <td>...</td>\n",
       "      <td>2.310868</td>\n",
       "      <td>4.973227</td>\n",
       "      <td>1.804057</td>\n",
       "      <td>3.259446</td>\n",
       "      <td>2.577447</td>\n",
       "      <td>1.298300</td>\n",
       "      <td>5.065431</td>\n",
       "      <td>1.187498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.256220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>4.582275</td>\n",
       "      <td>1.660137</td>\n",
       "      <td>3.289510</td>\n",
       "      <td>4.801923</td>\n",
       "      <td>2.190262</td>\n",
       "      <td>1.873648</td>\n",
       "      <td>3.533528</td>\n",
       "      <td>0.321808</td>\n",
       "      <td>20.026024</td>\n",
       "      <td>0.587416</td>\n",
       "      <td>...</td>\n",
       "      <td>2.613114</td>\n",
       "      <td>4.823492</td>\n",
       "      <td>1.957515</td>\n",
       "      <td>3.481168</td>\n",
       "      <td>2.479536</td>\n",
       "      <td>1.310285</td>\n",
       "      <td>5.184880</td>\n",
       "      <td>1.176876</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.689938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>4.883265</td>\n",
       "      <td>1.925244</td>\n",
       "      <td>3.156833</td>\n",
       "      <td>4.236933</td>\n",
       "      <td>2.167694</td>\n",
       "      <td>1.807244</td>\n",
       "      <td>4.102010</td>\n",
       "      <td>0.352488</td>\n",
       "      <td>19.286654</td>\n",
       "      <td>0.520876</td>\n",
       "      <td>...</td>\n",
       "      <td>3.064241</td>\n",
       "      <td>4.737156</td>\n",
       "      <td>2.297171</td>\n",
       "      <td>3.650515</td>\n",
       "      <td>2.560338</td>\n",
       "      <td>1.336107</td>\n",
       "      <td>5.260967</td>\n",
       "      <td>1.161684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.905931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>5.219643</td>\n",
       "      <td>2.206390</td>\n",
       "      <td>3.037402</td>\n",
       "      <td>3.234049</td>\n",
       "      <td>2.072338</td>\n",
       "      <td>1.807770</td>\n",
       "      <td>4.878557</td>\n",
       "      <td>0.421332</td>\n",
       "      <td>16.709980</td>\n",
       "      <td>0.447555</td>\n",
       "      <td>...</td>\n",
       "      <td>3.078428</td>\n",
       "      <td>4.628940</td>\n",
       "      <td>2.792172</td>\n",
       "      <td>4.003664</td>\n",
       "      <td>2.610692</td>\n",
       "      <td>1.389194</td>\n",
       "      <td>5.270358</td>\n",
       "      <td>1.144776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.109708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>5.487511</td>\n",
       "      <td>2.587518</td>\n",
       "      <td>2.873133</td>\n",
       "      <td>2.475744</td>\n",
       "      <td>2.162069</td>\n",
       "      <td>1.903919</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.444213</td>\n",
       "      <td>13.889608</td>\n",
       "      <td>0.376120</td>\n",
       "      <td>...</td>\n",
       "      <td>3.262271</td>\n",
       "      <td>4.480877</td>\n",
       "      <td>3.205117</td>\n",
       "      <td>4.315747</td>\n",
       "      <td>2.660851</td>\n",
       "      <td>1.443448</td>\n",
       "      <td>5.351387</td>\n",
       "      <td>1.136698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.247321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     MMM_LSTM  ABT_LSTM  ABBV_LSTM  ABMD_LSTM  ACN_LSTM  ATVI_LSTM  ADBE_LSTM  \\\n",
       "0    2.746678  1.263829   2.977300  10.950496  2.121233   1.582992   3.045472   \n",
       "1    2.604462  1.347925   2.837328  11.502473  2.076994   1.385383   3.458163   \n",
       "2    2.506720  1.532234   2.690357  11.359133  2.217411   1.456448   3.177202   \n",
       "3    2.556544  1.487930   2.586582  11.199201  2.315778   1.513432   3.145749   \n",
       "4    2.653760  1.448560   2.476370  10.926137  2.452366   1.431906   3.164338   \n",
       "5    2.789705  1.466732   2.292356  11.097188  2.597506   1.355233   3.199279   \n",
       "6    2.520596  1.498231   2.372945  10.901910  2.656395   1.284487   3.097516   \n",
       "7    2.456314  1.454131   2.275495  10.609546  2.834765   1.199835   3.044875   \n",
       "8    2.532359  1.444410   2.048051  10.361605  2.871272   1.100978   3.038062   \n",
       "9    2.371970  1.473193   2.097228   9.949480  2.899386   1.052868   2.904028   \n",
       "10   2.328750  1.463590   2.194467   9.686427  2.924464   0.978868   2.911077   \n",
       "11   2.173480  1.394488   2.222129   9.138044  3.019649   0.906515   2.911084   \n",
       "12   1.920112  1.346577   2.230808   8.712054  3.000086   0.992611   2.771694   \n",
       "13   2.357292  1.339398   1.948196   8.214464  3.053873   0.807891   2.865607   \n",
       "14   2.246773  1.342822   2.074416   7.930867  3.076565   0.756071   2.793038   \n",
       "15   2.343300  1.356081   2.338900   6.949923  3.089094   0.714931   2.766054   \n",
       "16   2.273836  1.344707   2.239627   6.325040  3.077646   0.722908   2.807404   \n",
       "17   2.311069  1.293249   2.279090   6.009716  3.084542   0.581665   2.709841   \n",
       "18   2.518391  1.202951   2.687167   6.176762  2.899903   0.628997   2.601374   \n",
       "19   2.728711  1.248739   2.528518   6.440399  2.839310   0.528580   2.676202   \n",
       "20   2.983467  1.170245   2.745014   5.487902  2.678045   0.450003   2.357194   \n",
       "21   3.039682  1.137266   2.888206   4.848337  2.500256   0.415078   2.228842   \n",
       "22   3.161940  1.071580   2.941781   5.295352  2.586976   0.305217   2.124664   \n",
       "23   3.374002  1.030071   2.799276   4.812072  2.630831   0.308008   2.222906   \n",
       "24   3.363330  1.119236   2.900210   5.062479  2.547870   0.316480   2.230299   \n",
       "25   3.672615  1.048164   2.877817   4.701971  2.415934   0.373778   2.094470   \n",
       "26   3.797816  0.898898   2.790826   5.299451  2.446560   0.242471   1.782888   \n",
       "27   3.776825  0.787825   2.962956   5.058153  2.537338   0.287806   1.968989   \n",
       "28   3.732041  0.803765   2.944781   5.164745  2.452312   0.553298   1.870477   \n",
       "29   3.740077  0.946909   2.860651   4.506537  2.475805   0.476938   1.973807   \n",
       "..        ...       ...        ...        ...       ...        ...        ...   \n",
       "169  7.716015  2.004672   3.552657   8.714144  4.389483   2.639385   4.409557   \n",
       "170  7.346168  1.899821   3.516581   8.940817  4.202414   2.665159   4.517550   \n",
       "171  7.462346  1.967792   3.551240   8.801637  4.100710   2.589188   4.628248   \n",
       "172  7.365163  2.017829   3.478335   8.954115  4.252169   2.531693   4.447756   \n",
       "173  6.985539  2.012576   3.468397   8.962796  4.083341   2.465862   4.211867   \n",
       "174  7.079862  2.072668   3.597387   8.756420  4.013752   2.534108   4.300268   \n",
       "175  7.325026  1.982349   3.483316   8.681897  4.068781   2.548849   4.104004   \n",
       "176  7.146482  2.077264   3.492620   8.400241  4.079584   2.463905   3.987910   \n",
       "177  7.037849  2.097354   3.460556   7.987897  3.837708   2.446741   3.849759   \n",
       "178  6.969994  2.063510   3.453405   8.373335  3.906883   2.481504   3.784415   \n",
       "179  6.946720  2.055064   3.510028   7.313595  3.807618   2.505655   3.595908   \n",
       "180  6.681860  2.094994   3.458711   7.422924  3.675675   2.505776   3.726813   \n",
       "181  6.331856  2.053636   3.475015   6.712495  3.541226   2.620948   3.721153   \n",
       "182  6.281549  2.137906   3.465244   6.316717  3.526549   2.566958   3.965447   \n",
       "183  5.736904  2.113547   3.434884   5.936811  3.351282   2.504764   3.900218   \n",
       "184  5.457012  2.023819   3.432133   5.739915  3.260415   2.466964   3.641037   \n",
       "185  5.260221  2.004555   3.484759   5.852886  3.036194   2.537002   3.881792   \n",
       "186  5.099674  1.878920   3.382051   5.663213  2.734810   2.474914   3.490659   \n",
       "187  4.844844  1.786943   3.432712   5.829678  2.504832   2.516272   3.388572   \n",
       "188  4.709681  1.802395   3.304519   6.026285  2.347301   2.342018   3.436087   \n",
       "189  4.367590  1.719529   3.290607   5.892690  2.331561   2.293547   3.467786   \n",
       "190  4.285939  1.687209   3.349431   5.932576  2.126344   2.237429   3.461668   \n",
       "191  4.234381  1.683043   3.339738   5.688180  2.022375   2.203481   3.488759   \n",
       "192  4.130336  1.539781   3.340353   5.775073  2.034177   2.145543   3.480060   \n",
       "193  4.166007  1.513977   3.318076   5.826199  2.050064   2.059565   2.787339   \n",
       "194  4.315480  1.475366   3.338932   5.178884  2.229365   1.978180   2.998095   \n",
       "195  4.582275  1.660137   3.289510   4.801923  2.190262   1.873648   3.533528   \n",
       "196  4.883265  1.925244   3.156833   4.236933  2.167694   1.807244   4.102010   \n",
       "197  5.219643  2.206390   3.037402   3.234049  2.072338   1.807770   4.878557   \n",
       "198  5.487511  2.587518   2.873133   2.475744  2.162069   1.903919   5.600000   \n",
       "\n",
       "     AMD_LSTM   AAP_LSTM  AES_LSTM  ...  ANSS_LSTM  ANTM_LSTM  AON_LSTM  \\\n",
       "0    0.236905   4.414233  0.454450  ...   2.496719   8.977930  3.036090   \n",
       "1    0.257399   3.069070  0.372114  ...   2.639326   8.816739  3.086694   \n",
       "2    0.243733   4.770209  0.415398  ...   2.791107   8.751988  3.047426   \n",
       "3    0.245023   4.023721  0.444935  ...   2.588377   8.489985  2.980619   \n",
       "4    0.239634   4.272506  0.446472  ...   2.543484   8.544436  2.982077   \n",
       "5    0.235208   4.868787  0.459655  ...   2.571283   8.408281  2.895457   \n",
       "6    0.236457   4.157387  0.459785  ...   2.600001   8.493305  2.948728   \n",
       "7    0.236943   4.529382  0.476904  ...   2.561957   7.885571  2.927686   \n",
       "8    0.234150   4.305710  0.490974  ...   2.441464   7.776992  2.874181   \n",
       "9    0.234060   4.820348  0.474304  ...   2.348216   7.493142  2.833179   \n",
       "10   0.231943   4.740590  0.477061  ...   2.295630   7.389724  2.831971   \n",
       "11   0.214102   4.112461  0.456521  ...   2.291628   7.365362  2.855068   \n",
       "12   0.191725   4.694531  0.461096  ...   2.283102   7.405556  2.819233   \n",
       "13   0.203085   4.794912  0.462295  ...   2.254327   7.203054  2.827205   \n",
       "14   0.199992   4.509519  0.484440  ...   2.227126   7.421291  2.785801   \n",
       "15   0.208402   4.315370  0.496845  ...   2.240983   7.567932  2.704467   \n",
       "16   0.231055   4.588702  0.519169  ...   2.269269   7.267528  2.678356   \n",
       "17   0.238287   4.820841  0.520136  ...   2.242296   7.194514  2.550920   \n",
       "18   0.243938   4.788873  0.527337  ...   2.088383   6.904310  2.367507   \n",
       "19   0.256797   3.905598  0.521383  ...   2.045553   6.689503  2.175531   \n",
       "20   0.260133   4.313820  0.538857  ...   1.799425   6.441620  1.939007   \n",
       "21   0.275046   4.396197  0.557376  ...   1.706838   6.149405  1.857300   \n",
       "22   0.287846   4.595329  0.563305  ...   1.625999   5.854003  1.783817   \n",
       "23   0.292874   4.997905  0.569763  ...   1.521136   5.740407  1.411998   \n",
       "24   0.275338   4.359016  0.587059  ...   1.644097   5.546580  1.519546   \n",
       "25   0.291582   4.073099  0.578791  ...   1.483976   5.254621  1.573419   \n",
       "26   0.287581   3.857609  0.569784  ...   1.393417   4.940042  1.449605   \n",
       "27   0.290458   2.805261  0.579130  ...   1.483398   4.720181  1.617377   \n",
       "28   0.318674   3.333326  0.583295  ...   1.427096   4.826407  1.433380   \n",
       "29   0.291063   3.237543  0.574267  ...   1.473566   4.775004  1.481441   \n",
       "..        ...        ...       ...  ...        ...        ...       ...   \n",
       "169  0.185497  12.872873  0.545758  ...   2.678195   5.998573  2.002035   \n",
       "170  0.195057  12.868512  0.531984  ...   2.657952   6.107925  2.020830   \n",
       "171  0.199579  13.359957  0.535028  ...   2.698160   6.166174  1.980290   \n",
       "172  0.201665  13.156663  0.537011  ...   2.651728   6.101252  2.025902   \n",
       "173  0.195546  13.279577  0.556876  ...   2.676324   6.263401  2.051124   \n",
       "174  0.196225  14.549615  0.558480  ...   2.637138   6.296924  2.215502   \n",
       "175  0.213708  15.649204  0.571184  ...   2.618462   6.238167  2.267833   \n",
       "176  0.206864  15.982629  0.566553  ...   2.715004   6.213355  2.188086   \n",
       "177  0.208366  15.815984  0.576861  ...   2.678911   6.149334  2.377474   \n",
       "178  0.224124  16.492923  0.566328  ...   2.638995   5.815475  2.304302   \n",
       "179  0.218806  17.219788  0.583090  ...   2.611610   5.582200  2.337840   \n",
       "180  0.226024  17.706643  0.610506  ...   2.658028   5.536940  2.248043   \n",
       "181  0.225081  17.305273  0.608555  ...   2.711180   5.360961  2.370515   \n",
       "182  0.227003  18.398152  0.610304  ...   2.696485   5.139799  2.308639   \n",
       "183  0.228450  17.343442  0.625456  ...   2.693791   4.964303  2.155359   \n",
       "184  0.229165  18.354647  0.628410  ...   2.699251   4.689444  2.217362   \n",
       "185  0.223442  18.632970  0.644550  ...   2.664533   4.721077  2.160532   \n",
       "186  0.241492  19.938052  0.648209  ...   2.591970   4.950129  1.914180   \n",
       "187  0.240558  19.193150  0.644076  ...   2.505128   4.994822  1.932842   \n",
       "188  0.258445  18.816818  0.632181  ...   2.439698   4.920035  1.792295   \n",
       "189  0.274186  20.718654  0.649987  ...   2.339554   4.933063  1.717406   \n",
       "190  0.290330  20.005320  0.646198  ...   2.342745   5.002521  1.846644   \n",
       "191  0.306321  19.833711  0.650585  ...   2.314530   4.947623  1.715738   \n",
       "192  0.319380  19.461400  0.638865  ...   2.269999   4.959281  1.745817   \n",
       "193  0.322474  18.386773  0.644650  ...   2.277900   4.830219  1.915125   \n",
       "194  0.306556  19.451953  0.630900  ...   2.310868   4.973227  1.804057   \n",
       "195  0.321808  20.026024  0.587416  ...   2.613114   4.823492  1.957515   \n",
       "196  0.352488  19.286654  0.520876  ...   3.064241   4.737156  2.297171   \n",
       "197  0.421332  16.709980  0.447555  ...   3.078428   4.628940  2.792172   \n",
       "198  0.444213  13.889608  0.376120  ...   3.262271   4.480877  3.205117   \n",
       "\n",
       "     AOS_LSTM  APA_LSTM  AIV_LSTM  AAPL_LSTM  AMAT_LSTM  APTV_LSTM  ADM_LSTM  \n",
       "0    3.588603  3.220512  1.146114   7.914326   0.708415        NaN  0.965052  \n",
       "1    3.499710  3.023498  1.032093   7.419336   0.761793        NaN  0.450033  \n",
       "2    3.315575  3.198543  1.145909   7.389762   0.723384        NaN  0.536544  \n",
       "3    3.277602  3.028910  1.106916   7.051504   0.764903        NaN  0.332982  \n",
       "4    3.333642  3.034087  1.189285   6.987285   0.843652        NaN  0.359546  \n",
       "5    3.329290  2.981835  1.173093   6.746178   0.838837        NaN  0.681497  \n",
       "6    3.217503  3.034510  1.184590   6.607978   0.842367        NaN  0.482274  \n",
       "7    3.169986  2.956802  1.174159   6.377526   0.881368        NaN  0.472192  \n",
       "8    3.162521  2.992117  1.175147   6.397264   0.898675        NaN  0.355063  \n",
       "9    2.988652  2.982576  1.188101   6.143486   0.949476        NaN  0.534804  \n",
       "10   2.780073  3.248208  1.195855   5.578852   0.931365        NaN  0.445584  \n",
       "11   2.620598  3.228076  1.205950   5.126293   0.932730        NaN  0.520785  \n",
       "12   2.547849  3.479354  1.207887   5.023874   0.955587        NaN  0.223810  \n",
       "13   2.206318  3.298482  1.213761   4.754395   1.030397        NaN  0.477746  \n",
       "14   1.935241  3.450245  1.192601   4.646911   1.031773        NaN  0.415236  \n",
       "15   1.910227  3.644313  1.159541   4.326182   1.041937        NaN  0.488241  \n",
       "16   1.802348  3.563947  1.121177   4.115644   1.043621        NaN  0.482004  \n",
       "17   1.879237  3.615629  1.065911   3.399671   1.065640        NaN  0.464902  \n",
       "18   1.654096  3.543355  1.023570   3.763044   1.085537        NaN  0.326605  \n",
       "19   1.789558  3.515821  1.007385   3.882660   1.231544        NaN  0.705204  \n",
       "20   1.347219  3.444860  1.034110   3.347233   1.349575        NaN  0.417349  \n",
       "21   1.277434  3.583659  1.030087   3.240056   1.373117        NaN  0.549303  \n",
       "22   1.377017  3.498826  0.998623   2.920755   1.412284        NaN  0.502515  \n",
       "23   1.302062  3.595644  1.017976   2.902311   1.529234        NaN  0.687156  \n",
       "24   1.290197  3.520003  0.896143   2.978411   1.574244        NaN  0.649101  \n",
       "25   1.461269  3.514529  0.940581   2.869073   1.618892        NaN  0.852723  \n",
       "26   1.433980  3.497093  0.964321   2.697359   1.701124        NaN  0.781207  \n",
       "27   1.257527  3.552906  0.940872   2.659901   1.774058        NaN  0.810649  \n",
       "28   1.391799  3.588790  1.006390   2.593900   1.757291        NaN  0.972307  \n",
       "29   1.463359  3.586575  0.960226   2.949771   1.789630        NaN  1.053880  \n",
       "..        ...       ...       ...        ...        ...        ...       ...  \n",
       "169  5.070417  4.261664  1.659961   3.830608   1.148936        NaN  3.106014  \n",
       "170  4.765053  4.239940  1.585587   3.610066   1.194368        NaN  3.050259  \n",
       "171  4.862331  4.277868  1.508472   3.625078   1.267123        NaN  3.291612  \n",
       "172  4.961339  4.215969  1.535045   3.706531   1.285716        NaN  3.329777  \n",
       "173  4.862581  4.156904  1.457688   3.553498   1.284250        NaN  3.507134  \n",
       "174  5.014201  4.115918  1.420992   3.686825   1.340262        NaN  3.497783  \n",
       "175  4.837426  4.060811  1.407787   3.613768   1.365421        NaN  3.405929  \n",
       "176  5.093752  3.926975  1.404183   3.541342   1.409262        NaN  3.583360  \n",
       "177  4.718745  3.965750  1.353355   3.496858   1.432676        NaN  3.863780  \n",
       "178  5.125374  3.967440  1.357178   3.636503   1.446568        NaN  3.805008  \n",
       "179  4.870176  3.961177  1.341660   3.824827   1.449295        NaN  3.915082  \n",
       "180  4.776369  3.832967  1.293183   3.672155   1.434632        NaN  4.107077  \n",
       "181  4.934159  3.757970  1.286796   3.814636   1.445718        NaN  4.245030  \n",
       "182  5.025581  3.653267  1.304969   3.686934   1.414441        NaN  4.175756  \n",
       "183  4.723551  3.335475  1.294060   3.918112   1.387179        NaN  4.172163  \n",
       "184  4.391881  3.283866  1.243571   4.049317   1.381131        NaN  4.464393  \n",
       "185  4.626076  3.215857  1.240915   4.155048   1.405677        NaN  4.531690  \n",
       "186  4.553236  2.958487  1.181020   4.373277   1.354341        NaN  4.557642  \n",
       "187  4.190211  2.759309  1.183408   4.294200   1.310246        NaN  4.307787  \n",
       "188  4.057402  2.647423  1.169968   4.398555   1.315898        NaN  4.329351  \n",
       "189  3.701914  2.602858  1.213056   4.361246   1.266175        NaN  4.472666  \n",
       "190  3.487907  2.625894  1.244063   4.426616   1.272437        NaN  4.311843  \n",
       "191  3.418020  2.370275  1.275843   4.481264   1.240368        NaN  4.379284  \n",
       "192  3.389740  2.526142  1.257741   4.787381   1.223807        NaN  4.024019  \n",
       "193  3.172297  2.577814  1.269659   4.867476   1.222636        NaN  4.220589  \n",
       "194  3.259446  2.577447  1.298300   5.065431   1.187498        NaN  4.256220  \n",
       "195  3.481168  2.479536  1.310285   5.184880   1.176876        NaN  3.689938  \n",
       "196  3.650515  2.560338  1.336107   5.260967   1.161684        NaN  2.905931  \n",
       "197  4.003664  2.610692  1.389194   5.270358   1.144776        NaN  2.109708  \n",
       "198  4.315747  2.660851  1.443448   5.351387   1.136698        NaN  2.247321  \n",
       "\n",
       "[199 rows x 53 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2.905931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2.109708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2.247321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "196  2.905931\n",
       "197  2.109708\n",
       "198  2.247321"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_log).tail(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MMM</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABMD</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ATVI</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>AMD</th>\n",
       "      <th>...</th>\n",
       "      <th>ANSS_LSTM</th>\n",
       "      <th>ANTM_LSTM</th>\n",
       "      <th>AON_LSTM</th>\n",
       "      <th>AOS_LSTM</th>\n",
       "      <th>APA_LSTM</th>\n",
       "      <th>AIV_LSTM</th>\n",
       "      <th>AAPL_LSTM</th>\n",
       "      <th>AMAT_LSTM</th>\n",
       "      <th>APTV_LSTM</th>\n",
       "      <th>ADM_LSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>5.210299</td>\n",
       "      <td>4.330293</td>\n",
       "      <td>1.355878</td>\n",
       "      <td>3.360956</td>\n",
       "      <td>5.760224</td>\n",
       "      <td>2.175936</td>\n",
       "      <td>2.036133</td>\n",
       "      <td>2.725390</td>\n",
       "      <td>0.294230</td>\n",
       "      <td>...</td>\n",
       "      <td>2.310868</td>\n",
       "      <td>4.973227</td>\n",
       "      <td>1.804057</td>\n",
       "      <td>3.259446</td>\n",
       "      <td>2.577447</td>\n",
       "      <td>1.298300</td>\n",
       "      <td>5.065431</td>\n",
       "      <td>1.187498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.256220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>5.682090</td>\n",
       "      <td>4.627899</td>\n",
       "      <td>1.356963</td>\n",
       "      <td>3.351274</td>\n",
       "      <td>5.767142</td>\n",
       "      <td>2.336475</td>\n",
       "      <td>1.967336</td>\n",
       "      <td>2.586573</td>\n",
       "      <td>0.288655</td>\n",
       "      <td>...</td>\n",
       "      <td>2.613114</td>\n",
       "      <td>4.823492</td>\n",
       "      <td>1.957515</td>\n",
       "      <td>3.481168</td>\n",
       "      <td>2.479536</td>\n",
       "      <td>1.310285</td>\n",
       "      <td>5.184880</td>\n",
       "      <td>1.176876</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.689938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>6.083995</td>\n",
       "      <td>4.910338</td>\n",
       "      <td>1.368321</td>\n",
       "      <td>3.321918</td>\n",
       "      <td>5.761588</td>\n",
       "      <td>2.513616</td>\n",
       "      <td>1.908270</td>\n",
       "      <td>2.444089</td>\n",
       "      <td>0.284957</td>\n",
       "      <td>...</td>\n",
       "      <td>3.064241</td>\n",
       "      <td>4.737156</td>\n",
       "      <td>2.297171</td>\n",
       "      <td>3.650515</td>\n",
       "      <td>2.560338</td>\n",
       "      <td>1.336107</td>\n",
       "      <td>5.260967</td>\n",
       "      <td>1.161684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.905931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date      AAPL       MMM       ABT      ABBV      ABMD       ACN  \\\n",
       "194  2016-01-06  5.210299  4.330293  1.355878  3.360956  5.760224  2.175936   \n",
       "195  2016-01-07  5.682090  4.627899  1.356963  3.351274  5.767142  2.336475   \n",
       "196  2016-01-08  6.083995  4.910338  1.368321  3.321918  5.761588  2.513616   \n",
       "\n",
       "         ATVI      ADBE       AMD  ...  ANSS_LSTM  ANTM_LSTM  AON_LSTM  \\\n",
       "194  2.036133  2.725390  0.294230  ...   2.310868   4.973227  1.804057   \n",
       "195  1.967336  2.586573  0.288655  ...   2.613114   4.823492  1.957515   \n",
       "196  1.908270  2.444089  0.284957  ...   3.064241   4.737156  2.297171   \n",
       "\n",
       "     AOS_LSTM  APA_LSTM  AIV_LSTM  AAPL_LSTM  AMAT_LSTM  APTV_LSTM  ADM_LSTM  \n",
       "194  3.259446  2.577447  1.298300   5.065431   1.187498        NaN  4.256220  \n",
       "195  3.481168  2.479536  1.310285   5.184880   1.176876        NaN  3.689938  \n",
       "196  3.650515  2.560338  1.336107   5.260967   1.161684        NaN  2.905931  \n",
       "\n",
       "[3 rows x 113 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final=df_all.reset_index().join(df_LSTM)\n",
    "df_final.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MMM</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABMD</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ATVI</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AAP</th>\n",
       "      <th>...</th>\n",
       "      <th>ANSS_LSTM</th>\n",
       "      <th>ANTM_LSTM</th>\n",
       "      <th>AON_LSTM</th>\n",
       "      <th>AOS_LSTM</th>\n",
       "      <th>APA_LSTM</th>\n",
       "      <th>AIV_LSTM</th>\n",
       "      <th>AAPL_LSTM</th>\n",
       "      <th>AMAT_LSTM</th>\n",
       "      <th>APTV_LSTM</th>\n",
       "      <th>ADM_LSTM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>5.210299</td>\n",
       "      <td>4.330293</td>\n",
       "      <td>1.355878</td>\n",
       "      <td>3.360956</td>\n",
       "      <td>5.760224</td>\n",
       "      <td>2.175936</td>\n",
       "      <td>2.036133</td>\n",
       "      <td>2.725390</td>\n",
       "      <td>0.294230</td>\n",
       "      <td>19.609404</td>\n",
       "      <td>...</td>\n",
       "      <td>2.310868</td>\n",
       "      <td>4.973227</td>\n",
       "      <td>1.804057</td>\n",
       "      <td>3.259446</td>\n",
       "      <td>2.577447</td>\n",
       "      <td>1.298300</td>\n",
       "      <td>5.065431</td>\n",
       "      <td>1.187498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.256220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07</th>\n",
       "      <td>5.682090</td>\n",
       "      <td>4.627899</td>\n",
       "      <td>1.356963</td>\n",
       "      <td>3.351274</td>\n",
       "      <td>5.767142</td>\n",
       "      <td>2.336475</td>\n",
       "      <td>1.967336</td>\n",
       "      <td>2.586573</td>\n",
       "      <td>0.288655</td>\n",
       "      <td>19.615478</td>\n",
       "      <td>...</td>\n",
       "      <td>2.613114</td>\n",
       "      <td>4.823492</td>\n",
       "      <td>1.957515</td>\n",
       "      <td>3.481168</td>\n",
       "      <td>2.479536</td>\n",
       "      <td>1.310285</td>\n",
       "      <td>5.184880</td>\n",
       "      <td>1.176876</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.689938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-08</th>\n",
       "      <td>6.083995</td>\n",
       "      <td>4.910338</td>\n",
       "      <td>1.368321</td>\n",
       "      <td>3.321918</td>\n",
       "      <td>5.761588</td>\n",
       "      <td>2.513616</td>\n",
       "      <td>1.908270</td>\n",
       "      <td>2.444089</td>\n",
       "      <td>0.284957</td>\n",
       "      <td>19.665678</td>\n",
       "      <td>...</td>\n",
       "      <td>3.064241</td>\n",
       "      <td>4.737156</td>\n",
       "      <td>2.297171</td>\n",
       "      <td>3.650515</td>\n",
       "      <td>2.560338</td>\n",
       "      <td>1.336107</td>\n",
       "      <td>5.260967</td>\n",
       "      <td>1.161684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.905931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL       MMM       ABT      ABBV      ABMD       ACN  \\\n",
       "Date                                                                     \n",
       "2016-01-06  5.210299  4.330293  1.355878  3.360956  5.760224  2.175936   \n",
       "2016-01-07  5.682090  4.627899  1.356963  3.351274  5.767142  2.336475   \n",
       "2016-01-08  6.083995  4.910338  1.368321  3.321918  5.761588  2.513616   \n",
       "\n",
       "                ATVI      ADBE       AMD        AAP  ...  ANSS_LSTM  \\\n",
       "Date                                                 ...              \n",
       "2016-01-06  2.036133  2.725390  0.294230  19.609404  ...   2.310868   \n",
       "2016-01-07  1.967336  2.586573  0.288655  19.615478  ...   2.613114   \n",
       "2016-01-08  1.908270  2.444089  0.284957  19.665678  ...   3.064241   \n",
       "\n",
       "            ANTM_LSTM  AON_LSTM  AOS_LSTM  APA_LSTM  AIV_LSTM  AAPL_LSTM  \\\n",
       "Date                                                                       \n",
       "2016-01-06   4.973227  1.804057  3.259446  2.577447  1.298300   5.065431   \n",
       "2016-01-07   4.823492  1.957515  3.481168  2.479536  1.310285   5.184880   \n",
       "2016-01-08   4.737156  2.297171  3.650515  2.560338  1.336107   5.260967   \n",
       "\n",
       "            AMAT_LSTM  APTV_LSTM  ADM_LSTM  \n",
       "Date                                        \n",
       "2016-01-06   1.187498        NaN  4.256220  \n",
       "2016-01-07   1.176876        NaN  3.689938  \n",
       "2016-01-08   1.161684        NaN  2.905931  \n",
       "\n",
       "[3 rows x 112 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred=df_final[-3:]\n",
    "df_pred=df_pred.set_index('Date')\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "####        STD la ###########\n",
    "\n",
    "df_pred.to_csv(r\"C:\\Users\\Geoffroy\\Desktop\\IT\\Projet Supélec\\Data_quandl\\LSTM_All_and_Pred_std25_Final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
